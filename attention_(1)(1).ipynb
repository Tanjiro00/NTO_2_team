{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "attention (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8bd864c2"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Распознавание текста\n",
        "\n",
        "## CRNN+CTC loss baseline\n",
        "\n",
        "В данном ноутбуке представлен baseline модели распознавания текста с помощью CRNN модели и CTC loss. Вы можете добавить новые аугментации или изменить структуру данной модели, или же попробовать совершенно новую архитектуру."
      ],
      "metadata": {
        "id": "768baa80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Установка и подгрузука библиотек"
      ],
      "metadata": {
        "id": "3aafd11a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка библиотек, под которым запускается данный бейзлайн."
      ],
      "metadata": {
        "id": "d1e9ec7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "kemSsCAlydcD",
        "outputId": "c8c1ae94-57b6-4dbd-fb4f-17c833d40aa0",
        "execution": {
          "iopub.status.busy": "2022-03-01T14:53:51.525524Z",
          "iopub.execute_input": "2022-03-01T14:53:51.526135Z",
          "iopub.status.idle": "2022-03-01T14:53:52.230575Z",
          "shell.execute_reply.started": "2022-03-01T14:53:51.526029Z",
          "shell.execute_reply": "2022-03-01T14:53:52.22983Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar  4 06:35:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    76W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.20.3\n",
        "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install opencv-python==4.5.2.52\n",
        "# !pip install matplotlib==3.4.2"
      ],
      "metadata": {
        "id": "5dab956f",
        "execution": {
          "iopub.status.busy": "2022-03-01T14:53:52.233766Z",
          "iopub.execute_input": "2022-03-01T14:53:52.233974Z",
          "iopub.status.idle": "2022-03-01T14:53:52.239606Z",
          "shell.execute_reply.started": "2022-03-01T14:53:52.233949Z",
          "shell.execute_reply": "2022-03-01T14:53:52.238925Z"
        },
        "trusted": true
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning "
      ],
      "metadata": {
        "id": "-6YN6s-CSYeC",
        "outputId": "3ae4d2d3-29ca-4130-f1f7-0502ba853b72",
        "execution": {
          "iopub.status.busy": "2022-03-01T14:53:52.774867Z",
          "iopub.execute_input": "2022-03-01T14:53:52.775421Z",
          "iopub.status.idle": "2022-03-01T14:54:01.278094Z",
          "shell.execute_reply.started": "2022-03-01T14:53:52.775391Z",
          "shell.execute_reply": "2022-03-01T14:54:01.277236Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2022.2.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.7.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Requirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (59.5.0)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-client"
      ],
      "metadata": {
        "id": "aFz8oIz7XIxD",
        "outputId": "9bc1494b-915a-4289-ae68-c585f8adb9fd",
        "execution": {
          "iopub.status.busy": "2022-03-01T14:54:01.28027Z",
          "iopub.execute_input": "2022-03-01T14:54:01.280555Z",
          "iopub.status.idle": "2022-03-01T14:54:14.596541Z",
          "shell.execute_reply.started": "2022-03-01T14:54:01.28052Z",
          "shell.execute_reply": "2022-03-01T14:54:14.595701Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.7/dist-packages (0.14.3)\n",
            "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.21.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.25.11)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.27)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.7.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.3.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (0.18.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.12 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (1.24.12)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.12->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.0)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (5.17.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.17.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (0.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.11.2)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.2)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.11.1)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.7.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.yandexcloud.net/datasouls-competitions/ai-nto-final-2022/data.zip\n",
        "!unzip -qq data.zip"
      ],
      "metadata": {
        "id": "7RqmuluSVpdE",
        "execution": {
          "iopub.status.busy": "2022-03-01T14:54:14.599759Z",
          "iopub.execute_input": "2022-03-01T14:54:14.599983Z",
          "iopub.status.idle": "2022-03-01T14:57:57.058774Z",
          "shell.execute_reply.started": "2022-03-01T14:54:14.599957Z",
          "shell.execute_reply": "2022-03-01T14:57:57.05657Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870ab4bc-b052-4802-d78c-0b1bea6e778e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-04 05:39:31--  https://storage.yandexcloud.net/datasouls-competitions/ai-nto-final-2022/data.zip\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6370765510 (5.9G) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   5.93G  54.7MB/s    in 1m 58s  \n",
            "\n",
            "2022-03-04 05:41:30 (51.5 MB/s) - ‘data.zip’ saved [6370765510/6370765510]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "e833b6fd",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:36:16.78737Z",
          "iopub.execute_input": "2022-03-01T23:36:16.787631Z",
          "iopub.status.idle": "2022-03-01T23:36:16.793868Z",
          "shell.execute_reply.started": "2022-03-01T23:36:16.787601Z",
          "shell.execute_reply": "2022-03-01T23:36:16.792915Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "shutil.copy('/content/drive/MyDrive/nto_final/train_recognition.zip', '/content')\n",
        "!unzip -qq train_recognition.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhDucx55b8fD",
        "outputId": "328a948a-c2f6-4444-8d86-b214c9c45620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
        "!unzip -qq IAM_Words.zip\n",
        "!\n",
        "!mkdir data\n",
        "!mkdir data/words\n",
        "!tar -xf IAM_Words/words.tgz -C data/words\n",
        "!mv IAM_Words/words.txt data"
      ],
      "metadata": {
        "id": "exFWX_UXc26-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_recognition/labels.csv')"
      ],
      "metadata": {
        "id": "OxHubUQ9gSE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
        "for line in words:\n",
        "    if line[0] == \"#\":\n",
        "        continue\n",
        "    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n",
        "        words_list.append(line)\n",
        "\n",
        "len(words_list)\n",
        "\n",
        "np.random.shuffle(words_list)\n"
      ],
      "metadata": {
        "id": "7SazqFC-gaBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_idx]\n",
        "test_samples = words_list[split_idx:]\n",
        "\n",
        "val_split_idx = int(0.5 * len(test_samples))\n",
        "validation_samples = test_samples[:val_split_idx]\n",
        "test_samples = test_samples[val_split_idx:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(\n",
        "    test_samples\n",
        ")\n",
        "\n",
        "print(f\"Total training samples: {len(train_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_samples)}\")\n",
        "print(f\"Total test samples: {len(test_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU1Zqyh7ggo7",
        "outputId": "43dcd012-3986-41e6-de9a-2dbf83e5a540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 86810\n",
            "Total validation samples: 4823\n",
            "Total test samples: 4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = os.path.join(base_path, \"words\")\n",
        "\n",
        "\n",
        "def get_image_paths_and_labels(samples):\n",
        "    paths = []\n",
        "    corrected_samples = []\n",
        "    for (i, file_line) in enumerate(samples):\n",
        "        line_split = file_line.strip()\n",
        "        line_split = line_split.split(\" \")\n",
        "\n",
        "        # Each line split will have this format for the corresponding image:\n",
        "        # part1/part1-part2/part1-part2-part3.png\n",
        "        image_name = line_split[0]\n",
        "        partI = image_name.split(\"-\")[0]\n",
        "        partII = image_name.split(\"-\")[1]\n",
        "        img_path = os.path.join(\n",
        "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
        "        )\n",
        "        if os.path.getsize(img_path):\n",
        "            paths.append(img_path)\n",
        "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
        "\n",
        "    return paths, corrected_samples\n",
        "\n",
        "\n",
        "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
        "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
        "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)\n"
      ],
      "metadata": {
        "id": "UZ_wPP9GghTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['file_name', 'text']"
      ],
      "metadata": {
        "id": "6LdDjRG6glcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_labels(labels):\n",
        "    cleaned_labels = []\n",
        "    for label in labels:\n",
        "        label = label.split(\" \")[-1].strip()\n",
        "        cleaned_labels.append(label)\n",
        "    return cleaned_labels\n",
        "\n",
        "\n",
        "validation_labels_cleaned = clean_labels(validation_labels)\n",
        "test_labels_cleaned = clean_labels(test_labels)\n",
        "train_labels_cleaned = clean_labels(train_labels)"
      ],
      "metadata": {
        "id": "AVtXEzTogl7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddd = []\n",
        "\n",
        "for i in range(len(train_labels_cleaned)):\n",
        "  ddd.append((\"/content/\" + train_img_paths[i], train_labels_cleaned[i]))\n",
        "\n",
        "column_names = ['file_name', 'text']\n",
        "df_to_save = pd.DataFrame(ddd, columns=column_names)"
      ],
      "metadata": {
        "id": "MDNqFeGAgnaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_save = pd.DataFrame(ddd, columns=column_names)"
      ],
      "metadata": {
        "id": "CCCTMHlkgowQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rrr = pd.concat([df, df_to_save], axis=0)"
      ],
      "metadata": {
        "id": "JwMQLHvLgqQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rrr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IkJr5cmNg_Fp",
        "outputId": "38dc86f1-82b1-4a09-9d45-cb54cb32ccb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-daed8911-08d7-45e5-af2f-89d08e576f73\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.png</td>\n",
              "      <td>дверку,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.png</td>\n",
              "      <td>открыл</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.png</td>\n",
              "      <td>Я</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.png</td>\n",
              "      <td>хвостом.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.png</td>\n",
              "      <td>вилял</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86803</th>\n",
              "      <td>/content/data/words/h07/h07-071/h07-071-08-00.png</td>\n",
              "      <td>industries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86804</th>\n",
              "      <td>/content/data/words/f04/f04-028/f04-028-03-05.png</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86805</th>\n",
              "      <td>/content/data/words/c04/c04-066/c04-066-04-06.png</td>\n",
              "      <td>charts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86806</th>\n",
              "      <td>/content/data/words/e07/e07-079/e07-079-01-02.png</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86807</th>\n",
              "      <td>/content/data/words/p03/p03-047/p03-047-02-10.png</td>\n",
              "      <td>anyway</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>335316 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-daed8911-08d7-45e5-af2f-89d08e576f73')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-daed8911-08d7-45e5-af2f-89d08e576f73 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-daed8911-08d7-45e5-af2f-89d08e576f73');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               file_name        text\n",
              "0                                                  0.png     дверку,\n",
              "1                                                  1.png      открыл\n",
              "2                                                  2.png           Я\n",
              "3                                                  3.png    хвостом.\n",
              "4                                                  4.png       вилял\n",
              "...                                                  ...         ...\n",
              "86803  /content/data/words/h07/h07-071/h07-071-08-00.png  industries\n",
              "86804  /content/data/words/f04/f04-028/f04-028-03-05.png           ,\n",
              "86805  /content/data/words/c04/c04-066/c04-066-04-06.png      charts\n",
              "86806  /content/data/words/e07/e07-079/e07-079-01-02.png           .\n",
              "86807  /content/data/words/p03/p03-047/p03-047-02-10.png      anyway\n",
              "\n",
              "[335316 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rrr.to_csv('new_labels.csv', index=False)"
      ],
      "metadata": {
        "id": "Om8UJfm7grnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Разделим трейн датасет на обучающую и валидационную подвыборки\n"
      ],
      "metadata": {
        "id": "b75fcf1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('./train/labels.json') as f:\n",
        "#     train_data = json.load(f)\n",
        "train_data = pd.read_csv(\"train_recognition/labels.csv\")\n",
        "#train_data = train_data.drop([\"base_image\"], 1)\n",
        "\n",
        "train_data = [(k, v) for k, v in train_data.values]\n",
        "shuffle(train_data)\n",
        "print('train len', len(train_data))\n",
        "\n",
        "split_coef = 0.75\n",
        "train_len = int(len(train_data)*split_coef)\n",
        "\n",
        "train_data_splitted = train_data[:train_len]\n",
        "val_data_splitted = train_data[train_len:]\n",
        "\n",
        "print('train len after split', len(train_data_splitted))\n",
        "print('val len after split', len(val_data_splitted))\n",
        "\n",
        "# with open('./train/train_labels_splitted.json', 'w') as f:\n",
        "#     json.dump(dict(train_data_splitted), f)\n",
        "    \n",
        "# with open('./train/val_labels_splitted.json', 'w') as f:\n",
        "#     json.dump(dict(val_data_splitted), f)"
      ],
      "metadata": {
        "id": "c622b5c4",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:37:53.100675Z",
          "iopub.execute_input": "2022-03-01T23:37:53.100988Z",
          "iopub.status.idle": "2022-03-01T23:37:53.67953Z",
          "shell.execute_reply.started": "2022-03-01T23:37:53.100953Z",
          "shell.execute_reply": "2022-03-01T23:37:53.67871Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed17b917-ece5-4fd4-f3a8-d5428934c674"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train len 248508\n",
            "train len after split 186381\n",
            "val len after split 62127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Зададим параметры обучения\n",
        "\n",
        "Здесь мы можем поправить конфиги обучения - задать размер батча, количество эпох, размер входных изображений, а также установить пути к датасетам."
      ],
      "metadata": {
        "id": "df5f5065"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [v for k, v in train_data]\n",
        "alphabet = ''.join(set(list(\"\".join(labels))))"
      ],
      "metadata": {
        "id": "b7NGjNsWV84U",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:37:57.977161Z",
          "iopub.execute_input": "2022-03-01T23:37:57.977713Z",
          "iopub.status.idle": "2022-03-01T23:37:58.123167Z",
          "shell.execute_reply.started": "2022-03-01T23:37:57.977676Z",
          "shell.execute_reply": "2022-03-01T23:37:58.122298Z"
        },
        "trusted": true
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_label = max([len(v) for v in labels])\n",
        "max_label"
      ],
      "metadata": {
        "id": "q2HX_pcCkpzp",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:37:58.821796Z",
          "iopub.execute_input": "2022-03-01T23:37:58.822597Z",
          "iopub.status.idle": "2022-03-01T23:37:58.853641Z",
          "shell.execute_reply.started": "2022-03-01T23:37:58.822547Z",
          "shell.execute_reply": "2022-03-01T23:37:58.852927Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b0f39a-d1be-4015-b868-b522fd0a3803"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = list(alphabet)\n",
        "a.sort()\n",
        "alphabet = ''.join(a)"
      ],
      "metadata": {
        "id": "k9gRR9FJ3BKF",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:37:59.479852Z",
          "iopub.execute_input": "2022-03-01T23:37:59.480093Z",
          "iopub.status.idle": "2022-03-01T23:37:59.484929Z",
          "shell.execute_reply.started": "2022-03-01T23:37:59.480059Z",
          "shell.execute_reply": "2022-03-01T23:37:59.483959Z"
        },
        "trusted": true
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet"
      ],
      "metadata": {
        "id": "dTD8jr8svyJL",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:38:00.716658Z",
          "iopub.execute_input": "2022-03-01T23:38:00.716895Z",
          "iopub.status.idle": "2022-03-01T23:38:00.722843Z",
          "shell.execute_reply.started": "2022-03-01T23:38:00.716868Z",
          "shell.execute_reply": "2022-03-01T23:38:00.722063Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ff80a1d0-2c13-4367-bf46-5959337440c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' !\"#%&\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "config_json = {\n",
        "    \"alphabet\": alphabet,\n",
        "    \"save_dir\": \"/content/drive/MyDrive/nto_final/train/experiments_attent/test\",\n",
        "    \"num_epochs\": 45,\n",
        "    \"image\": {\n",
        "        \"width\": 256,\n",
        "        \"height\": 50\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"root_path\": \"\",\n",
        "        \"json_path\": dict(train_data_splitted),\n",
        "        \"batch_size\": 16\n",
        "    },\n",
        "    \"val\": {\n",
        "        \"root_path\": \"\",\n",
        "        \"json_path\": dict(val_data_splitted),\n",
        "        \"batch_size\": 16\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "5fa07481",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:38:01.731979Z",
          "iopub.execute_input": "2022-03-01T23:38:01.732511Z",
          "iopub.status.idle": "2022-03-01T23:38:01.794747Z",
          "shell.execute_reply.started": "2022-03-01T23:38:01.732476Z",
          "shell.execute_reply": "2022-03-01T23:38:01.793999Z"
        },
        "trusted": true
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Теперь определим класс датасета (torch.utils.data.Dataset) и другие вспомогательные функции"
      ],
      "metadata": {
        "id": "9d8253e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook\n",
        "from skimage import transform, color, filters\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "tk-AqOuxZSrI",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:38:10.427211Z",
          "iopub.execute_input": "2022-03-01T23:38:10.42774Z",
          "iopub.status.idle": "2022-03-01T23:38:10.432874Z",
          "shell.execute_reply.started": "2022-03-01T23:38:10.427703Z",
          "shell.execute_reply": "2022-03-01T23:38:10.4322Z"
        },
        "trusted": true
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция которая помогает объединять картинки и таргет-текст в батч\n",
        "def collate_fn(batch):\n",
        "    images, texts, enc_texts = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    text_lens = torch.LongTensor([len(text) for text in texts])\n",
        "    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n",
        "    return images, texts, enc_pad_texts, text_lens\n",
        "\n",
        "\n",
        "def get_data_loader(\n",
        "    transforms, json_path, root_path, tokenizer, batch_size, drop_last\n",
        "):\n",
        "    dataset = OCRDataset(json_path, root_path, tokenizer, transforms)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=8,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, json_path, root_path, tokenizer, transform=None):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "#         with open(json_path, 'r') as f:\n",
        "#             data = json.load(f) --- костыль\n",
        "        data = json_path\n",
        "        self.data_len = len(data)\n",
        "        \n",
        "        self.images = []\n",
        "        self.texts = []\n",
        "        for img_name, text in tqdm.tqdm(data.items()):\n",
        "            self.texts.append(text)\n",
        "            image = img_name\n",
        "            self.images.append(image)\n",
        "        self.enc_texts = tokenizer.encode(self.texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path)\n",
        "        nnimg = Image.new(\"RGB\", image.size)\n",
        "        nnimg.paste(image)\n",
        "        image = nnimg\n",
        "        img = self.transform(image)\n",
        "        text = self.texts[idx]\n",
        "        enc_text = torch.LongTensor(self.enc_texts[idx])\n",
        "        return img, text, enc_text\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "81d7bd72",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:38:10.670275Z",
          "iopub.execute_input": "2022-03-01T23:38:10.670874Z",
          "iopub.status.idle": "2022-03-01T23:38:10.694876Z",
          "shell.execute_reply.started": "2022-03-01T23:38:10.670831Z",
          "shell.execute_reply": "2022-03-01T23:38:10.694162Z"
        },
        "trusted": true
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/train_recognition/images/75226.png')\n",
        "print(img.shape)\n",
        "Image.open('/content/train_recognition/images/75226.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "y2WjxBS7wvBc",
        "outputId": "60197054-2a1d-4769-d39e-53acbb158ca2"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(74, 72, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=72x74 at 0x7F83B35DCA90>"
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Здесь определен Токенайзер - вспопогательный класс, который преобразует текст в числа\n",
        "\n",
        "Разметка-текст с картинок преобразуется в числовое представление, на которых модель может учиться. Также может преобразовывать числовое предсказание модели обратно в текст."
      ],
      "metadata": {
        "id": "f9ab404a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV_TOKEN = '<OOV>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "SOS_TOKEN = '<sos>'\n",
        "\n",
        "\n",
        "def get_char_map(alphabet):\n",
        "    \"\"\"Make from string alphabet character2int dict.\n",
        "    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n",
        "    char_map = {value: idx + 3 for (idx, value) in enumerate(alphabet)}\n",
        "    char_map[SOS_TOKEN] = 0\n",
        "    char_map[OOV_TOKEN] = 1\n",
        "    char_map[EOS_TOKEN] = 2\n",
        "    return char_map\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Class for encoding and decoding string word to sequence of int\n",
        "    (and vice versa) using alphabet.\"\"\"\n",
        "\n",
        "    def __init__(self, alphabet):\n",
        "        self.char_map = get_char_map(alphabet)\n",
        "        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n",
        "\n",
        "    def encode(self, word_list):\n",
        "        \"\"\"Returns a list of encoded words (int).\"\"\"\n",
        "        enc_words = []\n",
        "        for word in word_list:\n",
        "            enc_words.append([self.char_map[SOS_TOKEN]] +\n",
        "                [self.char_map[char] if char in self.char_map\n",
        "                 else self.char_map[OOV_TOKEN]\n",
        "                 for char in word] + [self.char_map[EOS_TOKEN]] + [self.char_map[OOV_TOKEN]] * (23 - len(word))\n",
        "            )\n",
        "        return enc_words\n",
        "\n",
        "    def get_num_chars(self):\n",
        "        return len(self.char_map)\n",
        "\n",
        "    def decode(self, enc_word_list):\n",
        "        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n",
        "        repeating characters. Also skip out of vocabulary token.\"\"\"\n",
        "        dec_words = []\n",
        "        for word in enc_word_list:\n",
        "            word_chars = ''\n",
        "            for idx, char_enc in enumerate(word):\n",
        "                # skip if blank symbol, oov token or repeated characters\n",
        "                if (\n",
        "                    char_enc != self.char_map[OOV_TOKEN]\n",
        "                    and char_enc != self.char_map[SOS_TOKEN]\n",
        "                    and char_enc != self.char_map[EOS_TOKEN]\n",
        "                    # idx > 0 to avoid selecting [-1] item\n",
        "                    and not (idx > 0 and char_enc == word[idx - 1])\n",
        "                ):\n",
        "                    word_chars += self.rev_char_map[char_enc]\n",
        "            dec_words.append(word_chars)\n",
        "        return dec_words"
      ],
      "metadata": {
        "id": "b50de073",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:38:11.81959Z",
          "iopub.execute_input": "2022-03-01T23:38:11.820216Z",
          "iopub.status.idle": "2022-03-01T23:38:11.833282Z",
          "shell.execute_reply.started": "2022-03-01T23:38:11.820177Z",
          "shell.execute_reply": "2022-03-01T23:38:11.832462Z"
        },
        "trusted": true
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Accuracy в качестве метрики\n",
        "\n",
        "Accuracy измеряет долю предсказанных строк текста, которые полностью совпадают с таргет текстом."
      ],
      "metadata": {
        "id": "b11eaca2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo pip install fastwer\n",
        "# import fastwer"
      ],
      "metadata": {
        "id": "_oMjzgEpjUhD",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:11:00.256803Z",
          "iopub.execute_input": "2022-03-01T15:11:00.257081Z",
          "iopub.status.idle": "2022-03-01T15:11:00.260239Z",
          "shell.execute_reply.started": "2022-03-01T15:11:00.257044Z",
          "shell.execute_reply": "2022-03-01T15:11:00.259535Z"
        },
        "trusted": true
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "def levenshtein_distance(first, second):\n",
        "    distance = [[0 for _ in range(len(second) + 1)]\n",
        "                for _ in range(len(first) + 1)]\n",
        "    for i in range(len(first) + 1):\n",
        "        for j in range(len(second) + 1):\n",
        "            if i == 0:\n",
        "                distance[i][j] = j\n",
        "            elif j == 0:\n",
        "                distance[i][j] = i\n",
        "            else:\n",
        "                diag = distance[i - 1][j - 1] + (first[i - 1] != second[j - 1])\n",
        "                upper = distance[i - 1][j] + 1\n",
        "                left = distance[i][j - 1] + 1\n",
        "                distance[i][j] = min(diag, upper, left)\n",
        "    return distance[len(first)][len(second)]\n",
        "\n",
        "\n",
        "def cer(pred_texts, gt_texts):\n",
        "    assert len(pred_texts) == len(gt_texts)\n",
        "    lev_distances, num_gt_chars = 0, 0\n",
        "    for pred_text, gt_text in zip(pred_texts, gt_texts):\n",
        "        lev_distances += levenshtein_distance(pred_text, gt_text)\n",
        "        num_gt_chars += len(gt_text)\n",
        "    return lev_distances / (num_gt_chars + 1e-10)"
      ],
      "metadata": {
        "id": "IqsDrDK5jUhE",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:11:00.499909Z",
          "iopub.execute_input": "2022-03-01T15:11:00.50049Z",
          "iopub.status.idle": "2022-03-01T15:11:00.510097Z",
          "shell.execute_reply.started": "2022-03-01T15:11:00.500457Z",
          "shell.execute_reply": "2022-03-01T15:11:00.509186Z"
        },
        "trusted": true
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(y_true, y_pred):\n",
        "    scores = []\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        scores.append(true == pred)\n",
        "    avg_score = np.mean(scores)\n",
        "    return avg_score"
      ],
      "metadata": {
        "id": "e5c166f8",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:11:01.037221Z",
          "iopub.execute_input": "2022-03-01T15:11:01.037421Z",
          "iopub.status.idle": "2022-03-01T15:11:01.043076Z",
          "shell.execute_reply.started": "2022-03-01T15:11:01.037397Z",
          "shell.execute_reply": "2022-03-01T15:11:01.041067Z"
        },
        "trusted": true
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Аугментации\n",
        "\n",
        "Здесь мы задаем базовые аугментации для модели. Вы можете написать свои или использовать готовые библиотеки типа albumentations"
      ],
      "metadata": {
        "id": "7706c20d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(config_json['alphabet'])\n",
        "# data = OCRDataset(\"/content/train/train_labels_splitted.json\", \"/content/train/images\", tokenizer, get_train_transforms(config_json['image']['height'], config_json['image']['width']))"
      ],
      "metadata": {
        "id": "03KsyevGtiOo",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:38:15.660491Z",
          "iopub.execute_input": "2022-03-01T23:38:15.660768Z",
          "iopub.status.idle": "2022-03-01T23:38:15.667478Z",
          "shell.execute_reply.started": "2022-03-01T23:38:15.660736Z",
          "shell.execute_reply": "2022-03-01T23:38:15.666688Z"
        },
        "trusted": true
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalize:\n",
        "    def __call__(self, img):\n",
        "        img = img.astype(np.float32) / 255\n",
        "        return img\n",
        "\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, arr):\n",
        "        arr = torch.from_numpy(arr)\n",
        "        return arr\n",
        "\n",
        "\n",
        "class MoveChannels:\n",
        "    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n",
        "\n",
        "    def __init__(self, to_channels_first=True):\n",
        "        self.to_channels_first = to_channels_first\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.to_channels_first:\n",
        "            return np.moveaxis(image, -1, 0)\n",
        "        else:\n",
        "            return np.moveaxis(image, 0, -1)\n",
        "\n",
        "\n",
        "class ImageResize:\n",
        "    def __init__(self, height, width):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = cv2.resize(image, (self.width, self.height),\n",
        "                           interpolation=cv2.INTER_LINEAR)\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "def get_train_transforms(height, width):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((height, width)),\n",
        "        # MoveChannels(to_channels_first=True),\n",
        "        torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "        torchvision.transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "        torchvision.transforms.RandomRotation(5),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "\n",
        "    ])\n",
        "    return transforms\n",
        "\n",
        "\n",
        "def get_val_transforms(height, width):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((height, width)),\n",
        "        # MoveChannels(to_channels_first=True),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    return transforms"
      ],
      "metadata": {
        "id": "dd4ed911",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:18.577124Z",
          "iopub.execute_input": "2022-03-01T15:05:18.577793Z",
          "iopub.status.idle": "2022-03-01T15:05:18.594963Z",
          "shell.execute_reply.started": "2022-03-01T15:05:18.577744Z",
          "shell.execute_reply": "2022-03-01T15:05:18.594194Z"
        },
        "trusted": true
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Здесь определяем саму модель - CRNN\n",
        "\n",
        "Подробнее об архитектуре можно почитать в статье https://arxiv.org/abs/1507.05717"
      ],
      "metadata": {
        "id": "97e90f73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TPS_SpatialTransformerNetwork(nn.Module):\n",
        "    \"\"\" Rectification Network of RARE, namely TPS based STN \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_size, I_r_size, I_channel_num=1):\n",
        "        \"\"\" Based on RARE TPS\n",
        "        input:\n",
        "            batch_I: Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
        "            I_size : (height, width) of the input image I\n",
        "            I_r_size : (height, width) of the rectified image I_r\n",
        "            I_channel_num : the number of channels of the input image I\n",
        "        output:\n",
        "            batch_I_r: rectified image [batch_size x I_channel_num x I_r_height x I_r_width]\n",
        "        \"\"\"\n",
        "        super(TPS_SpatialTransformerNetwork, self).__init__()\n",
        "        self.F = F\n",
        "        self.I_size = I_size\n",
        "        self.I_r_size = I_r_size  # = (I_r_height, I_r_width)\n",
        "        self.I_channel_num = I_channel_num\n",
        "        self.LocalizationNetwork = LocalizationNetwork(self.F, self.I_channel_num)\n",
        "        self.GridGenerator = GridGenerator(self.F, self.I_r_size)\n",
        "\n",
        "    def forward(self, batch_I):\n",
        "        batch_C_prime = self.LocalizationNetwork(batch_I)  # batch_size x K x 2\n",
        "        build_P_prime = self.GridGenerator.build_P_prime(batch_C_prime)  # batch_size x n (= I_r_width x I_r_height) x 2\n",
        "        build_P_prime_reshape = build_P_prime.reshape([build_P_prime.size(0), self.I_r_size[0], self.I_r_size[1], 2])\n",
        "        batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border')\n",
        "\n",
        "        return batch_I_r\n",
        "\n",
        "\n",
        "class LocalizationNetwork(nn.Module):\n",
        "    \"\"\" Localization Network of RARE, which predicts C' (K x 2) from I (I_width x I_height) \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_channel_num):\n",
        "        super(LocalizationNetwork, self).__init__()\n",
        "        self.F = F\n",
        "        self.I_channel_num = I_channel_num\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.I_channel_num, out_channels=64, kernel_size=3, stride=1, padding=1,\n",
        "                      bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 64 x I_height/2 x I_width/2\n",
        "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 128 x I_height/4 x I_width/4\n",
        "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 256 x I_height/8 x I_width/8\n",
        "            nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n",
        "            nn.AdaptiveAvgPool2d(1)  # batch_size x 512\n",
        "        )\n",
        "\n",
        "        self.localization_fc1 = nn.Sequential(nn.Linear(512, 256), nn.ReLU(True))\n",
        "        self.localization_fc2 = nn.Linear(256, self.F * 2)\n",
        "\n",
        "        # Init fc2 in LocalizationNetwork\n",
        "        self.localization_fc2.weight.data.fill_(0)\n",
        "        \"\"\" see RARE paper Fig. 6 (a) \"\"\"\n",
        "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
        "        ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n",
        "        ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n",
        "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
        "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
        "        initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
        "        self.localization_fc2.bias.data = torch.from_numpy(initial_bias).float().view(-1)\n",
        "\n",
        "    def forward(self, batch_I):\n",
        "        \"\"\"\n",
        "        input:     batch_I : Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
        "        output:    batch_C_prime : Predicted coordinates of fiducial points for input batch [batch_size x F x 2]\n",
        "        \"\"\"\n",
        "        batch_size = batch_I.size(0)\n",
        "        features = self.conv(batch_I).view(batch_size, -1)\n",
        "        batch_C_prime = self.localization_fc2(self.localization_fc1(features)).view(batch_size, self.F, 2)\n",
        "        return batch_C_prime\n",
        "\n",
        "\n",
        "class GridGenerator(nn.Module):\n",
        "    \"\"\" Grid Generator of RARE, which produces P_prime by multipling T with P \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_r_size):\n",
        "        \"\"\" Generate P_hat and inv_delta_C for later \"\"\"\n",
        "        super(GridGenerator, self).__init__()\n",
        "        self.eps = 1e-6\n",
        "        self.I_r_height, self.I_r_width = I_r_size\n",
        "        self.F = F\n",
        "        self.C = self._build_C(self.F)  # F x 2\n",
        "        self.P = self._build_P(self.I_r_width, self.I_r_height)\n",
        "        self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
        "        self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n",
        "\n",
        "    def _build_C(self, F):\n",
        "        \"\"\" Return coordinates of fiducial points in I_r; C \"\"\"\n",
        "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
        "        ctrl_pts_y_top = -1 * np.ones(int(F / 2))\n",
        "        ctrl_pts_y_bottom = np.ones(int(F / 2))\n",
        "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
        "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
        "        C = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
        "        return C  # F x 2\n",
        "\n",
        "    def _build_inv_delta_C(self, F, C):\n",
        "        \"\"\" Return inv_delta_C which is needed to calculate T \"\"\"\n",
        "        hat_C = np.zeros((F, F), dtype=float)  # F x F\n",
        "        for i in range(0, F):\n",
        "            for j in range(i, F):\n",
        "                r = np.linalg.norm(C[i] - C[j])\n",
        "                hat_C[i, j] = r\n",
        "                hat_C[j, i] = r\n",
        "        np.fill_diagonal(hat_C, 1)\n",
        "        hat_C = (hat_C ** 2) * np.log(hat_C)\n",
        "        # print(C.shape, hat_C.shape)\n",
        "        delta_C = np.concatenate(  # F+3 x F+3\n",
        "            [\n",
        "                np.concatenate([np.ones((F, 1)), C, hat_C], axis=1),  # F x F+3\n",
        "                np.concatenate([np.zeros((2, 3)), np.transpose(C)], axis=1),  # 2 x F+3\n",
        "                np.concatenate([np.zeros((1, 3)), np.ones((1, F))], axis=1)  # 1 x F+3\n",
        "            ],\n",
        "            axis=0\n",
        "        )\n",
        "        inv_delta_C = np.linalg.inv(delta_C)\n",
        "        return inv_delta_C  # F+3 x F+3\n",
        "\n",
        "    def _build_P(self, I_r_width, I_r_height):\n",
        "        I_r_grid_x = (np.arange(-I_r_width, I_r_width, 2) + 1.0) / I_r_width  # self.I_r_width\n",
        "        I_r_grid_y = (np.arange(-I_r_height, I_r_height, 2) + 1.0) / I_r_height  # self.I_r_height\n",
        "        P = np.stack(  # self.I_r_width x self.I_r_height x 2\n",
        "            np.meshgrid(I_r_grid_x, I_r_grid_y),\n",
        "            axis=2\n",
        "        )\n",
        "        return P.reshape([-1, 2])  # n (= self.I_r_width x self.I_r_height) x 2\n",
        "\n",
        "    def _build_P_hat(self, F, C, P):\n",
        "        n = P.shape[0]  # n (= self.I_r_width x self.I_r_height)\n",
        "        P_tile = np.tile(np.expand_dims(P, axis=1), (1, F, 1))  # n x 2 -> n x 1 x 2 -> n x F x 2\n",
        "        C_tile = np.expand_dims(C, axis=0)  # 1 x F x 2\n",
        "        P_diff = P_tile - C_tile  # n x F x 2\n",
        "        rbf_norm = np.linalg.norm(P_diff, ord=2, axis=2, keepdims=False)  # n x F\n",
        "        rbf = np.multiply(np.square(rbf_norm), np.log(rbf_norm + self.eps))  # n x F\n",
        "        P_hat = np.concatenate([np.ones((n, 1)), P, rbf], axis=1)\n",
        "        return P_hat  # n x F+3\n",
        "\n",
        "    def build_P_prime(self, batch_C_prime):\n",
        "        \"\"\" Generate Grid from batch_C_prime [batch_size x F x 2] \"\"\"\n",
        "        batch_size = batch_C_prime.size(0)\n",
        "        batch_inv_delta_C = self.inv_delta_C.repeat(batch_size, 1, 1)\n",
        "        batch_P_hat = self.P_hat.repeat(batch_size, 1, 1)\n",
        "        batch_C_prime_with_zeros = torch.cat((batch_C_prime, torch.zeros(\n",
        "            batch_size, 3, 2).float().cuda()), dim=1)  # batch_size x F+3 x 2\n",
        "        batch_T = torch.bmm(batch_inv_delta_C, batch_C_prime_with_zeros)  # batch_size x F+3 x 2\n",
        "        batch_P_prime = torch.bmm(batch_P_hat, batch_T)  # batch_size x n x 2\n",
        "        return batch_P_prime  # batch_size x n x 2"
      ],
      "metadata": {
        "id": "EjBL30ipjUhJ",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:19.145657Z",
          "iopub.execute_input": "2022-03-01T15:05:19.145903Z",
          "iopub.status.idle": "2022-03-01T15:05:19.188242Z",
          "shell.execute_reply.started": "2022-03-01T15:05:19.145873Z",
          "shell.execute_reply": "2022-03-01T15:05:19.187367Z"
        },
        "trusted": true
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE: https://github.com/clovaai/deep-text-recognition-benchmark/blob/c2e28f5c0d30a81e884d358d0874c7a712e7515b/modules/prediction.py # noqa\n",
        "# + Some easy refactoring\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_cell = AttentionCell(input_size, hidden_size, num_classes)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.generator = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _char_to_onehot(self, input_char, onehot_dim=40):\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        batch_size = input_char.size(0)\n",
        "        one_hot = torch.FloatTensor(batch_size, onehot_dim).zero_().to(device)\n",
        "        one_hot = one_hot.scatter_(1, input_char, 1)\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, batch_H, text, is_train=True, batch_max_length=25):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            batch_H : contextual_feature H = hidden state of encoder. [batch_size x num_steps x contextual_feature_channels]\n",
        "            text : the text-index of each image. [batch_size x (max_length+1)]. +1 for [GO] token. text[:, 0] = [GO].\n",
        "        output: probability distribution at each step [batch_size x num_steps x num_classes]\n",
        "        \"\"\"\n",
        "        batch_size = batch_H.size(0)\n",
        "        num_steps = batch_max_length  # +1 for [s] at end of sentence.\n",
        "\n",
        "        output_hiddens = torch.FloatTensor(batch_size, num_steps, self.hidden_size).fill_(0).to(device)\n",
        "        hidden = (torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device),\n",
        "                  torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device))\n",
        "\n",
        "        if is_train:\n",
        "            for i in range(num_steps):\n",
        "                # one-hot vectors for a i-th char. in a batch\n",
        "                # print(num_steps)\n",
        "                # print(text.shape)\n",
        "                char_onehots = self._char_to_onehot(text[:, i], onehot_dim=self.num_classes)\n",
        "                # hidden : decoder's hidden s_{t-1}, batch_H : encoder's hidden H, char_onehots : one-hot(y_{t-1})\n",
        "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
        "                output_hiddens[:, i, :] = hidden[0]  # LSTM hidden index (0: hidden, 1: Cell)\n",
        "            probs = self.generator(output_hiddens)\n",
        "\n",
        "        else:\n",
        "            targets = torch.LongTensor(batch_size).fill_(0).to(device)  # [GO] token\n",
        "            probs = torch.FloatTensor(batch_size, num_steps, self.num_classes).fill_(0).to(device)\n",
        "\n",
        "            for i in range(num_steps):\n",
        "                char_onehots = self._char_to_onehot(targets, onehot_dim=self.num_classes)\n",
        "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
        "                probs_step = self.generator(hidden[0])\n",
        "                probs[:, i, :] = probs_step\n",
        "                _, next_input = probs_step.max(1)\n",
        "                targets = next_input\n",
        "\n",
        "        return probs  # batch_size x num_steps x num_classes\n",
        "\n",
        "\n",
        "class AttentionCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_embeddings):\n",
        "        super(AttentionCell, self).__init__()\n",
        "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)  # either i2i or h2h should have bias\n",
        "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.rnn = nn.LSTMCell(input_size + num_embeddings, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, prev_hidden, batch_H, char_onehots):\n",
        "        # [batch_size x num_encoder_/content/experiments_lstm_1/test/model-63-0.1780.ckptstep x num_channel] -> [batch_size x num_encoder_step x hidden_size]\n",
        "        batch_H_proj = self.i2h(batch_H)\n",
        "        prev_hidden_proj = self.h2h(prev_hidden[0]).unsqueeze(1)\n",
        "        e = self.score(torch.tanh(batch_H_proj + prev_hidden_proj))  # batch_size x num_encoder_step * 1\n",
        "\n",
        "        alpha = F.softmax(e, dim=1)\n",
        "        context = torch.bmm(alpha.permute(0, 2, 1), batch_H).squeeze(1)  # batch_size x num_channel\n",
        "        concat_context = torch.cat([context, char_onehots], 1)  # batch_size x (num_channel + num_embedding)\n",
        "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
        "        return cur_hidden, alpha"
      ],
      "metadata": {
        "id": "BfhX9Ho_quPK",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:19.708208Z",
          "iopub.execute_input": "2022-03-01T15:05:19.70866Z",
          "iopub.status.idle": "2022-03-01T15:05:19.728725Z",
          "shell.execute_reply.started": "2022-03-01T15:05:19.708628Z",
          "shell.execute_reply": "2022-03-01T15:05:19.727855Z"
        },
        "trusted": true
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "# !cd ctcdecode && pip install ."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T14:09:51.93945Z",
          "iopub.execute_input": "2022-02-19T14:09:51.94538Z",
          "iopub.status.idle": "2022-02-19T14:13:38.874416Z",
          "shell.execute_reply.started": "2022-02-19T14:09:51.94535Z",
          "shell.execute_reply": "2022-02-19T14:13:38.873309Z"
        },
        "trusted": true,
        "id": "fHHfNYNAdVwb"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Переходим к самому скрипту обучения - циклы трейна и валидации"
      ],
      "metadata": {
        "id": "f6df8f95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "g67qryLtdVwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7cda58-b40c-43b2-bf0b-622357669532"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 25):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x).permute(1, 0, 2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:36.563072Z",
          "iopub.execute_input": "2022-03-01T15:05:36.563607Z",
          "iopub.status.idle": "2022-03-01T15:05:36.572132Z",
          "shell.execute_reply.started": "2022-03-01T15:05:36.563569Z",
          "shell.execute_reply": "2022-03-01T15:05:36.57132Z"
        },
        "trusted": true,
        "id": "CWs_yGhjdVwd"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def test_beam_search_decoder_batch():\n",
        "        probs_seq = torch.FloatTensor([self.probs_seq1, self.probs_seq2])\n",
        "        decoder = ctcdecode.CTCBeamDecoder(\n",
        "            self.vocab_list, beam_width=2, blank_id=self.vocab_list.index(\"_\"), num_processes=24\n",
        "        )\n",
        "        beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
        "        output_str1 = tokenizer.decode(beam_results[0][0], self.vocab_list, out_seq_len[0][0])\n",
        "        output_str2 = tokenizer.decode(beam_results[1][0], self.vocab_list, out_seq_len[1][0])\n",
        "        del decoder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:37.002124Z",
          "iopub.execute_input": "2022-03-01T15:05:37.002573Z",
          "iopub.status.idle": "2022-03-01T15:05:37.008928Z",
          "shell.execute_reply.started": "2022-03-01T15:05:37.00254Z",
          "shell.execute_reply": "2022-03-01T15:05:37.00814Z"
        },
        "trusted": true,
        "id": "NTYDu6lMdVwd"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CTC Pytorch Models\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "import numpy as np\n",
        "from torch.utils.model_zoo import load_url\n",
        "\n",
        "rresnet_url = 'https://download.pytorch.org/models/resnet34-333f7ec4.pth'\n",
        "\n",
        "def downsample(chan_in, chan_out, stride, pad=0):\n",
        "    \n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(chan_in, chan_out, kernel_size=1, stride=stride, bias=False,\n",
        "                      padding=pad),\n",
        "            nn.BatchNorm2d(chan_out)\n",
        "            )\n",
        "\n",
        "# create a residual network, modify the downsampling as input is rectangular\n",
        "class CNN(nn.Module):\n",
        "    \"\"\" FeatureExtractor of CRNN (https://arxiv.org/pdf/1507.05717.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(CNN, self).__init__()\n",
        "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
        "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 64x16x50\n",
        "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 128x8x25\n",
        "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))  # 512x1x24\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "    \n",
        "    \n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True, num_layers=num_layers)\n",
        "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input : visual feature [batch_size x T x input_size]\n",
        "        output : contextual feature [batch_size x T x output_size]\n",
        "        \"\"\"\n",
        "        self.rnn.flatten_parameters()\n",
        "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
        "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
        "        return output\n",
        "        \n",
        "# SOURCE: https://github.com/clovaai/deep-text-recognition-benchmark/blob/c2e28f5c0d30a81e884d358d0874c7a712e7515b/model.py # noqa\n",
        "\"\"\"\n",
        "Copyright (c) 2019-present NAVER Corp.\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, hidden_size, num_class, batch_max_length=24, img_size=(100, 300)):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_max_length = batch_max_length\n",
        "#         # \"\"\" Transformation \"\"\"\n",
        "#         self.Transformation = TPS_SpatialTransformerNetwork(\n",
        "#                 F=20, I_size=img_size, I_r_size=img_size, I_channel_num=input_channel)\n",
        "        # else:\n",
        "        #     print('No Transformation module specified')\n",
        "\n",
        "        # \"\"\" FeatureExtraction \"\"\"\n",
        "        self.FeatureExtraction = CNN(input_channel, output_channel)\n",
        "        self.FeatureExtraction_output = output_channel  # int(imgH/16-1) * 512\n",
        "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
        "\n",
        "\n",
        "        \"\"\" Sequence modeling\"\"\"\n",
        "        self.SequenceModeling = nn.Sequential(\n",
        "            RNN(self.FeatureExtraction_output, hidden_size, hidden_size, num_layers=1),\n",
        "            RNN(hidden_size, hidden_size, hidden_size, num_layers=1)\n",
        "        )\n",
        "        self.SequenceModeling_output = hidden_size\n",
        "\n",
        "        \"\"\" Prediction \"\"\"\n",
        "        # if opt.Prediction == 'CTC':\n",
        "        #     self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
        "        # elif opt.Prediction == 'Attn':\n",
        "        self.pos_x = PositionalEncoding(self.SequenceModeling_output, max_len=100)\n",
        "        self.Prediction = Attention(self.SequenceModeling_output, hidden_size, num_class)\n",
        "#         self.emb = nn.Embedding(num_class, 256)\n",
        "        self.pos_y = PositionalEncoding(self.SequenceModeling_output, max_len=25)\n",
        "\n",
        "\n",
        "    def forward(self, input, text, is_train=True):\n",
        "        # \"\"\" Transformation stage \"\"\"\n",
        "        # if not self.stages['Trans'] == \"None\":\n",
        "#         input = self.Transformation(input)\n",
        "\n",
        "        \"\"\" Feature extraction stage \"\"\"\n",
        "        visual_feature = self.FeatureExtraction(input)\n",
        "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
        "        visual_feature = visual_feature.squeeze(3)\n",
        "\n",
        "        \"\"\" Sequence modeling stage \"\"\"\n",
        "        # if self.stages['Seq'] == 'BiLSTM':\n",
        "        contextual_feature = self.SequenceModeling(visual_feature)\n",
        "        # else:\n",
        "        #     contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
        "\n",
        "        \"\"\" Prediction stage \"\"\"\n",
        "        # if self.stages['Pred'] == 'CTC':\n",
        "        #     prediction = self.Prediction(contextual_feature.contiguous())\n",
        "        # else:\n",
        "        \n",
        "#         position = self.pos_x(contextual_feature.contiguous())\n",
        "#         if is_train:\n",
        "#             text = self.emb(text)\n",
        "#             text = self.pos_y(text)\n",
        "#         else:\n",
        "#             target = text\n",
        "        prediction = self.Prediction(contextual_feature, text, is_train, batch_max_length=self.batch_max_length)  # noqa\n",
        "        return prediction\n",
        "        \n",
        "    def best_path_decode(self, xb, y=None, mode='eval'):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if mode == 'eval':\n",
        "                out = self.forward(xb, None, is_train=False)\n",
        "            else:\n",
        "                out = self.forward(xb, y, is_train=True)\n",
        "            softmax_out = out.argmax(2).data.cpu().numpy()\n",
        "            char_list = []\n",
        "            for i in range(0, softmax_out.shape[0]):\n",
        "                dup_rm = softmax_out[i, :][np.insert(np.diff(softmax_out[i, :]).astype(np.bool), 0, True)]\n",
        "                dup_rm = dup_rm[dup_rm != 0]\n",
        "                char_list.append(dup_rm.astype(int))\n",
        "                \n",
        "        return char_list\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        images, texts, enc_pad_texts, text_lens = train_batch\n",
        "        images = images.to(DEVICE)\n",
        "        enc_pad_texts = torch.tensor(enc_pad_texts)\n",
        "        enc_pad_texts = enc_pad_texts.to(DEVICE)\n",
        "        x_hat = model(images, enc_pad_texts[:, :-1])\n",
        "        loss = criterion(x_hat.transpose(2, 1), enc_pad_texts[:, 1:])\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        images, texts, enc_text, _ = val_batch\n",
        "        images = images.to(DEVICE)\n",
        "        enc_text = torch.tensor(enc_text)\n",
        "        enc_text = enc_text.to(DEVICE)\n",
        "        x_hat = model(images, None, is_train=False)\n",
        "        loss = criterion(x_hat.transpose(2, 1), enc_text[:, 1:])\n",
        "        self.log('val_loss', loss)\n"
      ],
      "metadata": {
        "id": "76186574",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:37.45376Z",
          "iopub.execute_input": "2022-03-01T15:05:37.453972Z",
          "iopub.status.idle": "2022-03-01T15:05:37.487863Z",
          "shell.execute_reply.started": "2022-03-01T15:05:37.453947Z",
          "shell.execute_reply": "2022-03-01T15:05:37.486821Z"
        },
        "trusted": true
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install ruclip==0.0.1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:42.466205Z",
          "iopub.execute_input": "2022-03-01T15:05:42.466726Z",
          "iopub.status.idle": "2022-03-01T15:05:52.639304Z",
          "shell.execute_reply.started": "2022-03-01T15:05:42.466689Z",
          "shell.execute_reply": "2022-03-01T15:05:52.63838Z"
        },
        "trusted": true,
        "id": "bUYtucBSdVwg"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import ruclip\n",
        "# f\"All models: {list(ruclip.MODELS.keys())}\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:52.642406Z",
          "iopub.execute_input": "2022-03-01T15:05:52.642859Z",
          "iopub.status.idle": "2022-03-01T15:05:52.664448Z",
          "shell.execute_reply.started": "2022-03-01T15:05:52.642815Z",
          "shell.execute_reply": "2022-03-01T15:05:52.663665Z"
        },
        "trusted": true,
        "id": "AGM0-eo1dVwh"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import neptune.new as neptune\n",
        "run = neptune.init(\n",
        "    project=\"Voidname/baselinento\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0NTExN2M3ZC0yNTU1LTQ2NzEtYWRlZC1hY2ZmMDhmOTllMWIifQ==\",\n",
        ")  # your credentials"
      ],
      "metadata": {
        "id": "3UYiz3WjYV37",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:53.985726Z",
          "iopub.execute_input": "2022-03-01T15:05:53.986186Z",
          "iopub.status.idle": "2022-03-01T15:05:56.123596Z",
          "shell.execute_reply.started": "2022-03-01T15:05:53.986137Z",
          "shell.execute_reply": "2022-03-01T15:05:56.122734Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a939b5e-8c7d-4fc2-b47c-5336d5629a6e"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/Voidname/baselinento/e/BAS-270\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    'num_images': 'all',\n",
        "    'learning_rate': 0.001,\n",
        "    \"batch_size_train\": 64,\n",
        "    \"batch_size_val\": 64,\n",
        "    'n_epochs': 100,\n",
        "    'img_hight': 100,\n",
        "    'img_weight': 300,\n",
        "    'attention': False,\n",
        "    'backbone': 'resnet18',\n",
        "    'aug': [\n",
        "            \"A.Resize(height, width)\",\n",
        "        \"A.ChannelShuffle()\",\n",
        "        \"A.GridDistortion()\",\n",
        "        \"A.InvertImg(p=0.5)\",\n",
        "        \"A.HorizontalFlip(p=0.4)\",\n",
        "        \"A.ToGray(p=0.4)\",\n",
        "        \"A.RandomBrightnessContrast(p=0.5)\",\n",
        "    ],\n",
        "    \"choice\": \"best_path_decode\"\n",
        "\n",
        "}\n",
        "parameters = {\n",
        "    'model' : \"Attention CRNN\"\n",
        "}\n",
        "\n",
        "run['model/parameters'] = parameters"
      ],
      "metadata": {
        "id": "mDIJUaAMYg5i",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:56.126136Z",
          "iopub.execute_input": "2022-03-01T15:05:56.126462Z",
          "iopub.status.idle": "2022-03-01T15:05:56.132883Z",
          "shell.execute_reply.started": "2022-03-01T15:05:56.126422Z",
          "shell.execute_reply": "2022-03-01T15:05:56.131858Z"
        },
        "trusted": true
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "metadata": {
        "id": "LoicRizp2xAU",
        "execution": {
          "iopub.status.busy": "2022-03-01T15:05:57.128833Z",
          "iopub.execute_input": "2022-03-01T15:05:57.129101Z",
          "iopub.status.idle": "2022-03-01T15:05:57.133555Z",
          "shell.execute_reply.started": "2022-03-01T15:05:57.12907Z",
          "shell.execute_reply": "2022-03-01T15:05:57.132551Z"
        },
        "trusted": true
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partialmethod\n",
        "import tqdm\n",
        "\n",
        "def val_loop(data_loader, model, tokenizer, device):\n",
        "    acc_avg = AverageMeter()\n",
        "    cer_avg = AverageMeter()\n",
        "    for images, texts, enc_text, _ in tqdm.tqdm(data_loader):\n",
        "        batch_size = len(texts)\n",
        "        text_preds, pred = predict(images, model, tokenizer, device, choice='greedy_serch')\n",
        "        # cer_avg.update(fastwer.score(text_preds, texts, char_level=True), batch_size)\n",
        "        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n",
        "        cer_avg.update(cer(texts,text_preds))\n",
        "    # print(text_preds, texts, sep=' text pred \\n')\n",
        "    # print(pred)\n",
        "    run['evaluation/epoch/cer'].log(cer_avg.avg)\n",
        "    run['evaluation/epoch/cer'].log(cer_avg.avg)\n",
        "    print(text_preds)\n",
        "    print(texts)\n",
        "    print(f'Validation, acc: {acc_avg.avg:.4f}')\n",
        "    print(f'Validation, cer: {cer_avg.avg:.4f}')\n",
        "    return cer_avg.avg\n",
        "\n",
        "\n",
        "def train_loop(data_loader, model, criterion, optimizer, epoch, device='cuda'):\n",
        "    loss_avg = AverageMeter()\n",
        "    acc_avg = AverageMeter()\n",
        "    cer_avg = AverageMeter()\n",
        "    for images, texts, enc_pad_texts, text_lens in tqdm.tqdm(data_loader):\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        images = images.to(DEVICE)\n",
        "        # print(len(enc_pad_texts))\n",
        "        # print(enc_pad_texts[0].shape, enc_pad_texts[1].shape)\n",
        "        enc_pad_texts = torch.tensor(enc_pad_texts)\n",
        "        enc_pad_texts = enc_pad_texts.to(DEVICE)\n",
        "        batch_size = len(texts)\n",
        "        output = model(images, enc_pad_texts[:, :-1])\n",
        "        output_lenghts = torch.full(\n",
        "            size=(output.size(1),),\n",
        "            fill_value=output.size(0),\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        loss = criterion(output.transpose(2, 1), enc_pad_texts[:, 1:])\n",
        "        loss_avg.update(loss.item(), batch_size)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "        optimizer.step()\n",
        "        text_preds = predict(images, model, tokenizer, device, enc_pad_texts[:, :-1], mode='train', choice='greedy_serch')\n",
        "        # print(text_preds, texts)\n",
        "        cer_avg.update(cer(texts, text_preds[0]), batch_size)\n",
        "        acc_avg.update(get_accuracy(texts, text_preds[0]), batch_size)\n",
        "  \n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr = param_group['lr']\n",
        "    run['train/epoch/loss'].log(loss_avg.avg)\n",
        "    run['model/parameters/learning_rate'] = lr\n",
        "    run['train/epoch/accuracy'].log(acc_avg.avg)\n",
        "    run['train/epoch/cer'].log(cer_avg.avg)\n",
        "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n",
        "    print(f'Train, cer:{cer_avg.avg}')\n",
        "    return loss_avg.avg, output.argmax(2).data, texts, lr\n",
        "\n",
        "\n",
        "def predict(images, model, tokenizer, device, y=None, mode='eval', choice='greedy_serch'):\n",
        "    images = images.to(device)\n",
        "    if choice == 'best_path':\n",
        "        if mode == 'eval':\n",
        "            model.eval()\n",
        "            pred = np.array(model.best_path_decode(images))\n",
        "        else:\n",
        "            pred = np.array(model.best_path_decode(images, y, mode='train'))\n",
        "    elif choice == 'greedy_serch':\n",
        "        if mode == 'eval':\n",
        "            pred = model(images, None, is_train=False)\n",
        "            pred = pred.argmax(2).data.detach().cpu().numpy()\n",
        "        else:\n",
        "            pred = model(images, y, is_train=True)\n",
        "            pred = pred.argmax(2).data.detach().cpu().numpy()\n",
        "#     chars = list(tokenizer.char_map.keys())\n",
        "#     _ = chars.pop(chars.index(\"<BLANK>\"))\n",
        "#     with torch.no_grad():\n",
        "#         output = model(images).permute(1, 0, 2).cpu().detach().numpy()\n",
        "#     char_list = []\n",
        "#     for i in range(0, output.shape[0]):\n",
        "#         dup_rm = beam_search(output[i], chars)\n",
        "# #         print(dup_rm)\n",
        "#         char_list.append(dup_rm)\n",
        "    text_preds = tokenizer.decode(pred)\n",
        "    return text_preds, pred\n",
        "\n",
        "\n",
        "def get_loaders(tokenizer, config):\n",
        "    train_transforms = get_train_transforms(\n",
        "        height=config['image']['height'],\n",
        "        width=config['image']['width']\n",
        "    )\n",
        "    train_loader = get_data_loader(\n",
        "        json_path=config['train']['json_path'],\n",
        "        root_path=config['train']['root_path'],\n",
        "        transforms=train_transforms,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=config['train']['batch_size'],\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_transforms = get_val_transforms(\n",
        "        height=config['image']['height'],\n",
        "        width=config['image']['width']\n",
        "    )\n",
        "    val_loader = get_data_loader(\n",
        "        transforms=val_transforms,\n",
        "        json_path=config['val']['json_path'],\n",
        "        root_path=config['val']['root_path'],\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=config['val']['batch_size'],\n",
        "        drop_last=False\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    tokenizer = Tokenizer(config['alphabet'])\n",
        "    os.makedirs(config['save_dir'], exist_ok=True)\n",
        "    # train_loader, val_loader = get_loaders(tokenizer, config)\n",
        "\n",
        "    model = Model(3, 512, 512, tokenizer.get_num_chars())\n",
        "    #saves = torch.load(\"./train/experiments_attent/test/model-22-0.4357.ckpt\")\n",
        "    #model.load_state_dict(saves['model'])\n",
        "    model.to(DEVICE)\n",
        "    # model.load_pretrained_rnn()\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.003,\n",
        "                                  weight_decay=0.01)\n",
        "    #optimizer.load_state_dict(saves['lr'])\n",
        "    best_cer = np.inf\n",
        "    train_loader, val_loader = get_loaders(tokenizer, config_json)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        epochs=config['num_epochs'],\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        max_lr=0.01,\n",
        "    )\n",
        "    #scheduler.load_state_dict(saves['sched'])\n",
        "    #acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        loss_avg, pred, texts, lr = train_loop(train_loader, model, criterion, optimizer, epoch)\n",
        "        print(tokenizer.decode(pred.detach().cpu().numpy()))\n",
        "        print(texts)\n",
        "        cer_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n",
        "        scheduler.step(cer_avg)\n",
        "        if cer_avg < best_cer or epoch % 10 == 0:\n",
        "            best_cer = cer_avg\n",
        "            model_save_path = os.path.join(\n",
        "                config['save_dir'], f'model-{epoch}-{cer_avg:.4f}.ckpt')\n",
        "            torch.save({'model' : model.state_dict(),\n",
        "                       'epoch ': epoch,\n",
        "                        'lr': optimizer.state_dict(),\n",
        "                        'sched': scheduler.state_dict()\n",
        "                       }, model_save_path)\n",
        "            print('Model weights saved')"
      ],
      "metadata": {
        "id": "852fb92c",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:39:05.941495Z",
          "iopub.execute_input": "2022-03-01T23:39:05.941762Z",
          "iopub.status.idle": "2022-03-01T23:39:05.98479Z",
          "shell.execute_reply.started": "2022-03-01T23:39:05.941731Z",
          "shell.execute_reply": "2022-03-01T23:39:05.983698Z"
        },
        "trusted": true
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Model(3, 512, 512, tokenizer.get_num_chars())\n",
        "s = 0\n",
        "for el in a.parameters():\n",
        "  s += len(el)\n",
        "s"
      ],
      "metadata": {
        "id": "uoKjhCAKHQx8",
        "execution": {
          "iopub.status.busy": "2022-03-01T23:39:07.936995Z",
          "iopub.execute_input": "2022-03-01T23:39:07.937592Z",
          "iopub.status.idle": "2022-03-01T23:39:08.074781Z",
          "shell.execute_reply.started": "2022-03-01T23:39:07.937555Z",
          "shell.execute_reply": "2022-03-01T23:39:08.073959Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470c892e-0382-4586-8c3d-b950f33daf64"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50361"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(config_json)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-01T23:39:08.833897Z",
          "iopub.execute_input": "2022-03-01T23:39:08.834615Z",
          "iopub.status.idle": "2022-03-02T00:00:54.258821Z",
          "shell.execute_reply.started": "2022-03-01T23:39:08.83458Z",
          "shell.execute_reply": "2022-03-02T00:00:54.257327Z"
        },
        "trusted": true,
        "id": "kL2IgBIEdVwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07479285-c200-4533-8146-3931bde5a4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186381/186381 [00:00<00:00, 1036056.17it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 62127/62127 [00:00<00:00, 1062943.45it/s]\n",
            "  0%|          | 0/11649 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "100%|██████████| 11649/11649 [2:11:26<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0, Loss: 0.57932, LR: 0.0004000\n",
            "Train, cer:12308550765.316368\n",
            "['Донтрольный', 'shome', 'нерег.', 'поивалт', '.', 'отожаль', 'уамотнени,', 'of', 'titires', ',', 'gost', 'te', 'волоц.']\n",
            "('Контрольный', 'stones', 'нареч.', 'плавании', '.', 'слишиш', 'закончена;', 'of', \"Marshams'\", ',', 'great', 'be', 'полка.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3883/3883 [07:08<00:00,  9.07it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['порежне', 'demplomation', 'почель', 'the', 'the', 'anderted', 'потому', 'полод.', 'работа.', 'двугой', 'полом,', 'п', 'всего', 'sthines', '.']\n",
            "('перенесли', 'manufactured', 'полка.', 'the', 'this', 'animals', 'непогода', 'имеют', 'решить', 'радил-', 'малей-', '4.', 'Она', 'resistance', '.')\n",
            "Validation, acc: 0.1518\n",
            "Validation, cer: 0.8385\n",
            "Model weights saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 2438/11649 [27:24<1:43:01,  1.49it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image.open('/content/data/words/g01/g01-070/g01-070-02-00.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "7mw7rBh2z2L0",
        "outputId": "0b52a804-3f76-4fa7-a6ae-a343f74481eb"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=57x73 at 0x7F8398AF4C50>"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iimg = Image.open('/content/data/words/g01/g01-070/g01-070-02-00.png')\n",
        "print(np.array(iimg).shape)\n",
        "\n",
        "nnimg = Image.new(\"RGB\", iimg.size)\n",
        "print(np.array(nnimg).shape)\n",
        "nnimg.save('aaaaa1.jpg')\n",
        "nnimg.paste(iimg)\n",
        "print(np.array(nnimg).shape)\n",
        "nnimg.save('aaaaa2.jpg')\n",
        "\"\"\"\n",
        "nnimg.save('aaaaa.jpg')\n",
        "rgbimg.paste(img)\n",
        "rgbimg.save('foo.jpg')\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "XOLsKrn-yUE0",
        "outputId": "eeebf939-bc2d-4a84-c43c-319f8ea1f81e"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(73, 57)\n",
            "(73, 57, 3)\n",
            "(73, 57, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nnnimg.save('aaaaa.jpg')\\nrgbimg.paste(img)\\nrgbimg.save('foo.jpg')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        "os.listdir(\"./\")"
      ],
      "metadata": {
        "id": "Q5XD9nPL3BKi",
        "execution": {
          "iopub.status.busy": "2022-02-15T02:56:40.455701Z",
          "iopub.execute_input": "2022-02-15T02:56:40.456027Z",
          "iopub.status.idle": "2022-02-15T02:56:40.463463Z",
          "shell.execute_reply.started": "2022-02-15T02:56:40.455995Z",
          "shell.execute_reply": "2022-02-15T02:56:40.462639Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_dict = a.state_dict()\n",
        "# pretrained_dict = torch.load('../input/other-add/model-20-0.6432.ckpt')\n",
        "# pretrained_dict = {'rnn.{}'.format(\".\".join(k.split('.')[1:])): v for k, v in pretrained_dict.items() if 'rnn.{}'.format(\".\".join(k.split('.')[1:])) in model_dict}"
      ],
      "metadata": {
        "id": "ROipvoHMjUhR",
        "execution": {
          "iopub.status.busy": "2022-02-15T02:56:40.464961Z",
          "iopub.execute_input": "2022-02-15T02:56:40.465316Z",
          "iopub.status.idle": "2022-02-15T02:56:40.472318Z",
          "shell.execute_reply.started": "2022-02-15T02:56:40.46528Z",
          "shell.execute_reply": "2022-02-15T02:56:40.471539Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Запускаем обучение!"
      ],
      "metadata": {
        "id": "c960ad3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mx46QlFAjUhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.chdir(r'/kaggle/working')"
      ],
      "metadata": {
        "id": "KTtT8h5OjUhS",
        "execution": {
          "iopub.status.busy": "2022-02-19T21:18:17.509087Z",
          "iopub.execute_input": "2022-02-19T21:18:17.509791Z",
          "iopub.status.idle": "2022-02-19T21:18:17.514532Z",
          "shell.execute_reply.started": "2022-02-19T21:18:17.509755Z",
          "shell.execute_reply": "2022-02-19T21:18:17.513448Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = os.listdir(\"./train/experiments_attent/test\")"
      ],
      "metadata": {
        "id": "qZ3hvAwVjUhS",
        "execution": {
          "iopub.status.busy": "2022-03-02T00:01:03.03784Z",
          "iopub.execute_input": "2022-03-02T00:01:03.03813Z",
          "iopub.status.idle": "2022-03-02T00:01:03.044156Z",
          "shell.execute_reply.started": "2022-03-02T00:01:03.038096Z",
          "shell.execute_reply": "2022-03-02T00:01:03.0433Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths"
      ],
      "metadata": {
        "id": "wiB57-Am3BKl",
        "execution": {
          "iopub.status.busy": "2022-03-02T00:01:03.444919Z",
          "iopub.execute_input": "2022-03-02T00:01:03.445167Z",
          "iopub.status.idle": "2022-03-02T00:01:03.450682Z",
          "shell.execute_reply.started": "2022-03-02T00:01:03.445138Z",
          "shell.execute_reply": "2022-03-02T00:01:03.44991Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_score(paths):\n",
        "    epochs = [int(path.split('-')[1]) for path in paths]\n",
        "    max_ep_ind = epochs.index(max(epochs))\n",
        "    return paths[max_ep_ind]"
      ],
      "metadata": {
        "id": "AH3RtD5C3BKl",
        "execution": {
          "iopub.status.busy": "2022-03-02T00:01:04.355394Z",
          "iopub.execute_input": "2022-03-02T00:01:04.355843Z",
          "iopub.status.idle": "2022-03-02T00:01:04.362125Z",
          "shell.execute_reply.started": "2022-03-02T00:01:04.355807Z",
          "shell.execute_reply": "2022-03-02T00:01:04.359606Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "path = get_max_score(paths)\n",
        "print(path)\n",
        "FileLink(f\"./train/experiments_attent/test/{path}\")"
      ],
      "metadata": {
        "id": "rS1AVdZhjUhS",
        "execution": {
          "iopub.status.busy": "2022-03-02T00:01:05.348443Z",
          "iopub.execute_input": "2022-03-02T00:01:05.348703Z",
          "iopub.status.idle": "2022-03-02T00:01:05.359032Z",
          "shell.execute_reply.started": "2022-03-02T00:01:05.348673Z",
          "shell.execute_reply": "2022-03-02T00:01:05.358075Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Создание предсказаний для public-датасета\n",
        "\n",
        "Сначала определим класс для создания предсказаний"
      ],
      "metadata": {
        "id": "8bd864c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceTransform:\n",
        "    def __init__(self, height, width):\n",
        "        self.transforms = get_val_transforms(height, width)\n",
        "\n",
        "    def __call__(self, images):\n",
        "        transformed_images = []\n",
        "        for image in images:\n",
        "            image = self.transforms(image)\n",
        "            transformed_images.append(image)\n",
        "        transformed_tensor = torch.stack(transformed_images, 0)\n",
        "        return transformed_tensor\n",
        "\n",
        "\n",
        "class OcrPredictor:\n",
        "    def __init__(self, model_path, config, device='cuda'):\n",
        "        self.tokenizer = Tokenizer(config['alphabet'])\n",
        "        self.device = torch.device(device)\n",
        "        # load model\n",
        "        self.model = Model(3, 512, 512, tokenizer.get_num_chars())\n",
        "        self.model.load_state_dict(torch.load(model_path)['model'])\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.transforms = InferenceTransform(\n",
        "            height=config['image']['height'],\n",
        "            width=config['image']['width'],\n",
        "        )\n",
        "\n",
        "    def __call__(self, images):\n",
        "        if isinstance(images, (list, tuple)):\n",
        "            one_image = False\n",
        "        else:\n",
        "            images = [images]\n",
        "            one_image = True\n",
        "#         else:\n",
        "#             raise Exception(f\"Input must contain np.ndarray, \"\n",
        "#                             f\"tuple or list, found {type(images)}.\")\n",
        "\n",
        "        images = self.transforms(images)\n",
        "        pred = predict(images, self.model, self.tokenizer, self.device)\n",
        "\n",
        "        if one_image:\n",
        "            return pred[0]\n",
        "        else:\n",
        "            return pred"
      ],
      "metadata": {
        "id": "cb4859a0",
        "execution": {
          "iopub.status.busy": "2022-02-19T14:13:48.381756Z",
          "iopub.execute_input": "2022-02-19T14:13:48.382672Z",
          "iopub.status.idle": "2022-02-19T14:13:48.393563Z",
          "shell.execute_reply.started": "2022-02-19T14:13:48.382631Z",
          "shell.execute_reply": "2022-02-19T14:13:48.392815Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем OCR predictor"
      ],
      "metadata": {
        "id": "61e1490d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = OcrPredictor(\n",
        "    model_path=f'../input/other-add/attention-42-0.0866.ckpt',\n",
        "    config=config_json\n",
        ")"
      ],
      "metadata": {
        "id": "b754fe0d",
        "execution": {
          "iopub.status.busy": "2022-02-19T14:13:48.394449Z",
          "iopub.execute_input": "2022-02-19T14:13:48.396144Z",
          "iopub.status.idle": "2022-02-19T14:13:54.497491Z",
          "shell.execute_reply.started": "2022-02-19T14:13:48.396075Z",
          "shell.execute_reply": "2022-02-19T14:13:54.496772Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим несколько предсказаний и создадим финальный json"
      ],
      "metadata": {
        "id": "86b80f31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data_splitted[1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:27:35.659572Z",
          "iopub.execute_input": "2022-02-16T07:27:35.660235Z",
          "iopub.status.idle": "2022-02-16T07:27:35.668436Z",
          "shell.execute_reply.started": "2022-02-16T07:27:35.660185Z",
          "shell.execute_reply": "2022-02-16T07:27:35.667332Z"
        },
        "trusted": true,
        "id": "ZPuUb5uVdVwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "IBekpZeXdVwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_json = {}\n",
        "right_answers = []\n",
        "\n",
        "count = 0\n",
        "print_images = True\n",
        "for img_name, target in val_data_splitted:\n",
        "    img = Image.open(f'./train/images/{img_name}')\n",
        "\n",
        "    pred = predictor(img)\n",
        "    pred_json[img_name] = pred\n",
        "    if pred[0] == target:\n",
        "        right_answers.append(img_name)\n",
        "\n",
        "    if print_images:\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "        print('Prediction: ', predictor(img))\n",
        "        print(\"Target: \", target)\n",
        "        count += 1\n",
        "\n",
        "    if count > 30:\n",
        "        print_images = False"
      ],
      "metadata": {
        "id": "16a91332",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-02-19T14:13:54.499733Z",
          "iopub.execute_input": "2022-02-19T14:13:54.50015Z",
          "iopub.status.idle": "2022-02-19T14:20:19.423795Z",
          "shell.execute_reply.started": "2022-02-19T14:13:54.500113Z",
          "shell.execute_reply": "2022-02-19T14:20:19.421904Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AugCNN(nn.Module):\n",
        "    def __init__(self, images, transform=None):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join('./train/images', self.images[idx])\n",
        "        image = Image.open(path)\n",
        "        if self.transform:\n",
        "            return self.transform(image)\n",
        "        return image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T14:33:00.658235Z",
          "iopub.execute_input": "2022-02-19T14:33:00.6585Z",
          "iopub.status.idle": "2022-02-19T14:33:00.665414Z",
          "shell.execute_reply.started": "2022-02-19T14:33:00.658469Z",
          "shell.execute_reply": "2022-02-19T14:33:00.664713Z"
        },
        "trusted": true,
        "id": "0ExJ9HFPdVwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test = train_test_split(right_answers, test_size=0.2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T14:33:02.366389Z",
          "iopub.execute_input": "2022-02-19T14:33:02.36735Z",
          "iopub.status.idle": "2022-02-19T14:33:02.376424Z",
          "shell.execute_reply.started": "2022-02-19T14:33:02.367301Z",
          "shell.execute_reply": "2022-02-19T14:33:02.375538Z"
        },
        "trusted": true,
        "id": "zjrk4lxodVwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_val_transforms(height=None, width=None):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((height, width)),\n",
        "        # MoveChannels(to_channels_first=True),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    return transforms"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T14:33:03.517621Z",
          "iopub.execute_input": "2022-02-19T14:33:03.518553Z",
          "iopub.status.idle": "2022-02-19T14:33:03.524494Z",
          "shell.execute_reply.started": "2022-02-19T14:33:03.51848Z",
          "shell.execute_reply": "2022-02-19T14:33:03.523682Z"
        },
        "trusted": true,
        "id": "Lo7IEJH6dVwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = get_val_transforms(100, 300)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T14:33:04.199691Z",
          "iopub.execute_input": "2022-02-19T14:33:04.200443Z",
          "iopub.status.idle": "2022-02-19T14:33:04.204677Z",
          "shell.execute_reply.started": "2022-02-19T14:33:04.200399Z",
          "shell.execute_reply": "2022-02-19T14:33:04.203843Z"
        },
        "trusted": true,
        "id": "E0mshANXdVwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T15:39:24.700786Z",
          "iopub.execute_input": "2022-02-19T15:39:24.701379Z",
          "iopub.status.idle": "2022-02-19T15:39:24.707007Z",
          "shell.execute_reply.started": "2022-02-19T15:39:24.70134Z",
          "shell.execute_reply": "2022-02-19T15:39:24.706223Z"
        },
        "trusted": true,
        "id": "9XYBSEvgdVwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AugCNN(x_train, transform)\n",
        "test_dataset = AugCNN(x_test, transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T14:33:06.118362Z",
          "iopub.execute_input": "2022-02-19T14:33:06.119054Z",
          "iopub.status.idle": "2022-02-19T14:33:06.123827Z",
          "shell.execute_reply.started": "2022-02-19T14:33:06.119013Z",
          "shell.execute_reply": "2022-02-19T14:33:06.122912Z"
        },
        "trusted": true,
        "id": "FYlMirhKdVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Auger_CNN = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, 1),\n",
        "    nn.Conv2d(3, 16, 1),\n",
        "    nn.Conv2d(16, 32, 1),\n",
        "    nn.Conv2d(32, 16, 1),\n",
        "    nn.Conv2d(16, 3, 1)\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T17:54:00.659965Z",
          "iopub.execute_input": "2022-02-19T17:54:00.660449Z",
          "iopub.status.idle": "2022-02-19T17:54:00.669108Z",
          "shell.execute_reply.started": "2022-02-19T17:54:00.66041Z",
          "shell.execute_reply": "2022-02-19T17:54:00.668306Z"
        },
        "trusted": true,
        "id": "-tu_Xq9-dVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(data_loader, model, criterion, optimizer, epoch, device='cuda'):\n",
        "    loss_avg = AverageMeter()\n",
        "    for images in tqdm.tqdm_notebook(data_loader):\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        images = images.to(DEVICE)\n",
        "        aug_img = model(images)\n",
        "        loss = criterion(aug_img, images)\n",
        "        loss_avg.update(loss.item(), 64)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "        optimizer.step()\n",
        "        # print(text_preds, texts)\n",
        "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f} --- Train')\n",
        "    \n",
        "    \n",
        "def val_loop(data_loader, model, criterion, epoch, device='cuda'):\n",
        "    loss_avg = AverageMeter()\n",
        "    for images in data_loader:\n",
        "        model.eval()\n",
        "        images = images.to(DEVICE)\n",
        "        aug_img = model(images)\n",
        "        loss = criterion(aug_img, images)\n",
        "        loss_avg.update(loss.item(), 64)\n",
        "        # print(text_preds, texts)\n",
        "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f} --- Test')\n",
        "    return loss_avg.avg\n",
        "\n",
        "    \n",
        "def train(EPOCH=30, save='./'):\n",
        "    # train_loader, val_loader = get_loaders(tokenizer, config)\n",
        "\n",
        "    model = Auger_CNN\n",
        "    model.to(DEVICE)\n",
        "    # model.load_pretrained_rnn()\n",
        "    criterion = torch.nn.MSELoss().to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001,\n",
        "                                  weight_decay=0.01)\n",
        "#     optimizer.load_state_dict(saves['lr'])\n",
        "#     scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "#         optimizer,\n",
        "#         epochs=config['num_epochs'],\n",
        "#         steps_per_epoch=len(train_loader),\n",
        "#         max_lr=0.01,\n",
        "#     )\n",
        "#     scheduler.load_state_dict(saves['sched'])\n",
        "#     acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n",
        "    for epoch in tqdm.tqdm_notebook(range(EPOCH)):\n",
        "        loss_avg = train_loop(train_loader, model, criterion, optimizer, epoch)\n",
        "        loss_avg = val_loop(train_loader, model, criterion, epoch)\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save(model.state_dict(), f'./model_augmet_{loss_avg:.4f}.ckpt')\n",
        "            print('Model weights saved')\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T17:54:11.130792Z",
          "iopub.execute_input": "2022-02-19T17:54:11.131432Z",
          "iopub.status.idle": "2022-02-19T17:54:11.145265Z",
          "shell.execute_reply.started": "2022-02-19T17:54:11.13139Z",
          "shell.execute_reply": "2022-02-19T17:54:11.144462Z"
        },
        "trusted": true,
        "id": "8PomRDxgdVw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model = train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-19T17:54:12.303401Z",
          "iopub.execute_input": "2022-02-19T17:54:12.304117Z",
          "iopub.status.idle": "2022-02-19T18:17:23.165916Z",
          "shell.execute_reply.started": "2022-02-19T17:54:12.304079Z",
          "shell.execute_reply": "2022-02-19T18:17:23.165128Z"
        },
        "trusted": true,
        "id": "0iJXAApqdVw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QHFfyrl9dVw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраням submission json с предсказаниями"
      ],
      "metadata": {
        "id": "7f5f6e91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/prediction_HTR.json', 'w') as f:\n",
        "    json.dump(pred_json, f)"
      ],
      "metadata": {
        "id": "90b2452c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"./train\")"
      ],
      "metadata": {
        "id": "OVq1PJcL3BKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yH_KYyhe3BKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xME0FmbTdVw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VAE image preprocessing**"
      ],
      "metadata": {
        "id": "I0TcoB08dVw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test = train_test_split(right_answers, test_size=0.2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:36:57.779707Z",
          "iopub.execute_input": "2022-02-16T07:36:57.780531Z",
          "iopub.status.idle": "2022-02-16T07:36:57.792333Z",
          "shell.execute_reply.started": "2022-02-16T07:36:57.780485Z",
          "shell.execute_reply": "2022-02-16T07:36:57.791266Z"
        },
        "trusted": true,
        "id": "UUZqr5GTdVw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_dataset(nn.Module):\n",
        "    def __init__(self, paths, dir_path, transform=None):\n",
        "        self.paths = paths\n",
        "        self.transform = transform\n",
        "        self.dir = dir_path\n",
        "    \n",
        "    def __len__(self):\n",
        "        return(len(self.paths))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join(self.dir, self.paths[idx])\n",
        "        img = Image.open(path)\n",
        "        if self.transform == None:\n",
        "            return img\n",
        "        return self.transform(img)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:36:57.793745Z",
          "iopub.execute_input": "2022-02-16T07:36:57.794547Z",
          "iopub.status.idle": "2022-02-16T07:36:57.80556Z",
          "shell.execute_reply.started": "2022-02-16T07:36:57.794499Z",
          "shell.execute_reply": "2022-02-16T07:36:57.804564Z"
        },
        "trusted": true,
        "id": "is9lMUPJdVw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_val_transforms(height=None, width=None):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((height, width)),\n",
        "        # MoveChannels(to_channels_first=True),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    return transforms"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:36:57.808482Z",
          "iopub.execute_input": "2022-02-16T07:36:57.809677Z",
          "iopub.status.idle": "2022-02-16T07:36:57.817142Z",
          "shell.execute_reply.started": "2022-02-16T07:36:57.809629Z",
          "shell.execute_reply": "2022-02-16T07:36:57.815985Z"
        },
        "trusted": true,
        "id": "gdaBJkv9dVw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = get_val_transforms(100, 300)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:36:57.818799Z",
          "iopub.execute_input": "2022-02-16T07:36:57.819387Z",
          "iopub.status.idle": "2022-02-16T07:36:57.828886Z",
          "shell.execute_reply.started": "2022-02-16T07:36:57.819261Z",
          "shell.execute_reply": "2022-02-16T07:36:57.827763Z"
        },
        "trusted": true,
        "id": "TOESJ9vRdVw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VAE_dataset(x_train, \"./train/images\", transform)\n",
        "test_dataset = VAE_dataset(x_test, \"./train/images\", transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:36:57.831527Z",
          "iopub.execute_input": "2022-02-16T07:36:57.831795Z",
          "iopub.status.idle": "2022-02-16T07:36:57.841248Z",
          "shell.execute_reply.started": "2022-02-16T07:36:57.831764Z",
          "shell.execute_reply": "2022-02-16T07:36:57.83985Z"
        },
        "trusted": true,
        "id": "WIDdt1T3dVw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE MODEL"
      ],
      "metadata": {
        "id": "6WKuNoimdVw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vector-Quantization for the VQ-VAE itself.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def vq_loss(inputs, embedded, commitment=0.25):\n",
        "    \"\"\"\n",
        "    Compute the codebook and commitment losses for an\n",
        "    input-output pair from a VQ layer.\n",
        "    \"\"\"\n",
        "    return (torch.mean(torch.pow(inputs.detach() - embedded, 2)) +\n",
        "            commitment * torch.mean(torch.pow(inputs - embedded.detach(), 2)))\n",
        "\n",
        "\n",
        "class VQ(nn.Module):\n",
        "    \"\"\"\n",
        "    A vector quantization layer.\n",
        "    This layer takes continuous inputs and produces a few\n",
        "    different types of outputs, including a discretized\n",
        "    output, a commitment loss, a codebook loss, etc.\n",
        "    Args:\n",
        "        num_channels: the depth of the input Tensors.\n",
        "        num_latents: the number of latent values in the\n",
        "          dictionary to choose from.\n",
        "        dead_rate: the number of forward passes after\n",
        "          which a dictionary entry is considered dead if\n",
        "          it has not been used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, num_latents, dead_rate=100):\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.num_latents = num_latents\n",
        "        self.dead_rate = dead_rate\n",
        "\n",
        "        self.dictionary = nn.Parameter(torch.randn(num_latents, num_channels))\n",
        "        self.usage_count = nn.Parameter(dead_rate * torch.ones(num_latents).long(),\n",
        "                                        requires_grad=False)\n",
        "        self._last_batch = None\n",
        "\n",
        "    def embed(self, idxs):\n",
        "        \"\"\"\n",
        "        Convert encoded indices into embeddings.\n",
        "        Args:\n",
        "            idxs: an [N x H x W] or [N] Tensor.\n",
        "        Returns:\n",
        "            An [N x H x W x C] or [N x C] Tensor.\n",
        "        \"\"\"\n",
        "        embedded = F.embedding(idxs, self.dictionary)\n",
        "        if len(embedded.shape) == 4:\n",
        "            # NHWC to NCHW\n",
        "            embedded = embedded.permute(0, 3, 1, 2).contiguous()\n",
        "        return embedded\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Apply vector quantization.\n",
        "        If the module is in training mode, this will also\n",
        "        update the usage tracker and re-initialize dead\n",
        "        dictionary entries.\n",
        "        Args:\n",
        "            inputs: the input Tensor. Either [N x C] or\n",
        "              [N x C x H x W].\n",
        "        Returns:\n",
        "            A tuple (embedded, embedded_pt, idxs):\n",
        "              embedded: the new [N x C x H x W] Tensor\n",
        "                which passes gradients to the dictionary.\n",
        "              embedded_pt: like embedded, but with a\n",
        "                passthrough gradient estimator. Gradients\n",
        "                through this pass directly to the inputs.\n",
        "              idxs: a [N x H x W] Tensor of Longs\n",
        "                indicating the chosen dictionary entries.\n",
        "        \"\"\"\n",
        "        channels_last = inputs\n",
        "        if len(inputs.shape) == 4:\n",
        "            # NCHW to NHWC\n",
        "            channels_last = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        diffs = embedding_distances(self.dictionary, channels_last)\n",
        "        idxs = torch.argmin(diffs, dim=-1)\n",
        "        embedded = self.embed(idxs)\n",
        "        embedded_pt = embedded.detach() + (inputs - inputs.detach())\n",
        "\n",
        "        if self.training:\n",
        "            self._update_tracker(idxs)\n",
        "            self._last_batch = channels_last.detach()\n",
        "\n",
        "        return embedded, embedded_pt, idxs\n",
        "\n",
        "    def revive_dead_entries(self, inputs=None):\n",
        "        \"\"\"\n",
        "        Use the dictionary usage tracker to re-initialize\n",
        "        entries that aren't being used often.\n",
        "        Args:\n",
        "          inputs: a batch of inputs from which random\n",
        "            values are sampled for new entries. If None,\n",
        "            the previous input to forward() is used.\n",
        "        \"\"\"\n",
        "        if inputs is None:\n",
        "            assert self._last_batch is not None, ('cannot revive dead entries until a batch has ' +\n",
        "                                                  'been run')\n",
        "            inputs = self._last_batch\n",
        "        counts = self.usage_count.detach().cpu().numpy()\n",
        "        new_dictionary = None\n",
        "        inputs_numpy = None\n",
        "        for i, count in enumerate(counts):\n",
        "            if count:\n",
        "                continue\n",
        "            if new_dictionary is None:\n",
        "                new_dictionary = self.dictionary.detach().cpu().numpy()\n",
        "            if inputs_numpy is None:\n",
        "                inputs_numpy = inputs.detach().cpu().numpy().reshape([-1, inputs.shape[-1]])\n",
        "            new_dictionary[i] = random.choice(inputs_numpy)\n",
        "            counts[i] = self.dead_rate\n",
        "        if new_dictionary is not None:\n",
        "            dict_tensor = torch.from_numpy(new_dictionary).to(self.dictionary.device)\n",
        "            counts_tensor = torch.from_numpy(counts).to(self.usage_count.device)\n",
        "            self.dictionary.data.copy_(dict_tensor)\n",
        "            self.usage_count.data.copy_(counts_tensor)\n",
        "\n",
        "    def _update_tracker(self, idxs):\n",
        "        raw_idxs = set(idxs.detach().cpu().numpy().flatten())\n",
        "        update = -np.ones([self.num_latents], dtype=np.int)\n",
        "        for idx in raw_idxs:\n",
        "            update[idx] = self.dead_rate\n",
        "        self.usage_count.data.add_(torch.from_numpy(update).to(self.usage_count.device).long())\n",
        "        self.usage_count.data.clamp_(0, self.dead_rate)\n",
        "\n",
        "\n",
        "def embedding_distances(dictionary, tensor):\n",
        "    \"\"\"\n",
        "    Compute distances between every embedding in a\n",
        "    dictionary and every vector in a Tensor.\n",
        "    This will not generate a huge intermediate Tensor,\n",
        "    unlike the naive implementation.\n",
        "    Args:\n",
        "        dictionary: a [D x C] Tensor.\n",
        "        tensor: a [... x C] Tensor.\n",
        "    Returns:\n",
        "        A [... x D] Tensor of distances.\n",
        "    \"\"\"\n",
        "    dict_norms = torch.sum(torch.pow(dictionary, 2), dim=-1)\n",
        "    tensor_norms = torch.sum(torch.pow(tensor, 2), dim=-1)\n",
        "\n",
        "    # Work-around for https://github.com/pytorch/pytorch/issues/18862.\n",
        "    exp_tensor = tensor[..., None].view(-1, tensor.shape[-1], 1)\n",
        "    exp_dict = dictionary[None].expand(exp_tensor.shape[0], *dictionary.shape)\n",
        "    dots = torch.bmm(exp_dict, exp_tensor)[..., 0]\n",
        "    dots = dots.view(*tensor.shape[:-1], dots.shape[-1])\n",
        "\n",
        "    return -2 * dots + dict_norms + tensor_norms[..., None]\n",
        "\"\"\"\n",
        "An implementation of the hierarchical VQ-VAE.\n",
        "See https://arxiv.org/abs/1906.00446.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from .vq import VQ, vq_loss\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    An abstract VQ-VAE encoder, which takes input Tensors,\n",
        "    shrinks them, and quantizes the result.\n",
        "    Sub-classes should overload the encode() method.\n",
        "    Args:\n",
        "        num_channels: the number of channels in the latent\n",
        "          codebook.\n",
        "        num_latents: the number of entries in the latent\n",
        "          codebook.\n",
        "        kwargs: arguments to pass to the VQ layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, num_latents, **kwargs):\n",
        "        super().__init__()\n",
        "        self.vq = VQ(num_channels, num_latents, **kwargs)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Encode a Tensor before the VQ layer.\n",
        "        Args:\n",
        "            x: the input Tensor.\n",
        "        Returns:\n",
        "            A Tensor with the correct number of output\n",
        "              channels (according to self.vq).\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply the encoder.\n",
        "        See VQ.forward() for return values.\n",
        "        \"\"\"\n",
        "        return self.vq(self.encode(x))\n",
        "\n",
        "\n",
        "class QuarterEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    The encoder from the original VQ-VAE paper that cuts\n",
        "    the dimensions down by a factor of 4 in both\n",
        "    directions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, num_latents, **kwargs):\n",
        "        super().__init__(out_channels, num_latents, **kwargs)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 4, stride=2)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 4, stride=2)\n",
        "        self.residual1 = _make_residual(out_channels)\n",
        "        self.residual2 = _make_residual(out_channels)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Padding is uneven, so we make the right and\n",
        "        # bottom more padded arbitrarily.\n",
        "        x = F.pad(x, (1, 2, 1, 2))\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.pad(x, (1, 2, 1, 2))\n",
        "        x = self.conv2(x)\n",
        "        x = x + self.residual1(x)\n",
        "        x = x + self.residual2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HalfEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    An encoder that cuts the input size in half in both\n",
        "    dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, num_latents, **kwargs):\n",
        "        super().__init__(out_channels, num_latents, **kwargs)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1)\n",
        "        self.residual1 = _make_residual(out_channels)\n",
        "        self.residual2 = _make_residual(out_channels)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x + self.residual1(x)\n",
        "        x = x + self.residual2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    An abstract VQ-VAE decoder, which takes a stack of\n",
        "    (differently-sized) input Tensors and produces a\n",
        "    predicted output Tensor.\n",
        "    Sub-classes should overload the forward() method.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Apply the decoder to a list of inputs.\n",
        "        Args:\n",
        "            inputs: a sequence of input Tensors. There may\n",
        "              be more than one in the case of a hierarchy,\n",
        "              in which case the top levels come first.\n",
        "        Returns:\n",
        "            A decoded Tensor.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class QuarterDecoder(Decoder):\n",
        "    \"\"\"\n",
        "    The decoder from the original VQ-VAE paper that\n",
        "    upsamples the dimensions by a factor of 4 in both\n",
        "    directions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.residual1 = _make_residual(in_channels)\n",
        "        self.residual2 = _make_residual(in_channels)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert len(inputs) == 1\n",
        "        x = inputs[0]\n",
        "        x = x + self.residual1(x)\n",
        "        x = x + self.residual2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HalfDecoder(Decoder):\n",
        "    \"\"\"\n",
        "    A decoder that upsamples by a factor of 2 in both\n",
        "    dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.residual1 = _make_residual(in_channels)\n",
        "        self.residual2 = _make_residual(in_channels)\n",
        "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert len(inputs) == 1\n",
        "        x = inputs[0]\n",
        "        x = x + self.residual1(x)\n",
        "        x = x + self.residual2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HalfQuarterDecoder(Decoder):\n",
        "    \"\"\"\n",
        "    A decoder that takes two inputs. The first one is\n",
        "    upsampled by a factor of two, and then combined with\n",
        "    the second input which is further upsampled by a\n",
        "    factor of four.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.residual1 = _make_residual(in_channels)\n",
        "        self.residual2 = _make_residual(in_channels)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels * 2, in_channels, 3, padding=1)\n",
        "        self.residual3 = _make_residual(in_channels)\n",
        "        self.residual4 = _make_residual(in_channels)\n",
        "        self.conv3 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
        "        self.conv4 = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert len(inputs) == 2\n",
        "\n",
        "        # Upsample the top input to match the shape of the\n",
        "        # bottom input.\n",
        "        x = inputs[0]\n",
        "        x = x + self.residual1(x)\n",
        "        x = x + self.residual2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Mix together the bottom and top inputs.\n",
        "        x = torch.cat([x, inputs[1]], dim=1)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x + self.residual3(x)\n",
        "        x = x + self.residual4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    A complete VQ-VAE hierarchy.\n",
        "    There are N encoders, stored from the bottom level to\n",
        "    the top level, and N decoders stored from top to\n",
        "    bottom.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoders, decoders):\n",
        "        super().__init__()\n",
        "        assert len(encoders) == len(decoders)\n",
        "        self.encoders = encoders\n",
        "        self.decoders = decoders\n",
        "        for i, enc in enumerate(encoders):\n",
        "            self.add_module('encoder_%d' % i, enc)\n",
        "        for i, dec in enumerate(decoders):\n",
        "            self.add_module('decoder_%d' % i, dec)\n",
        "\n",
        "    def forward(self, inputs, commitment=0.25):\n",
        "        \"\"\"\n",
        "        Compute training losses for a batch of inputs.\n",
        "        Args:\n",
        "            inputs: the input Tensor. If this is a Tensor\n",
        "              of integers, then cross-entropy loss will be\n",
        "              used for the final decoder. Otherwise, MSE\n",
        "              will be used.\n",
        "            commitment: the commitment loss coefficient.\n",
        "        Returns:\n",
        "            A dict of Tensors, containing at least:\n",
        "              loss: the total training loss.\n",
        "              losses: the MSE/log-loss from each decoder.\n",
        "              reconstructions: a reconstruction Tensor\n",
        "                from each decoder.\n",
        "              embedded: outputs from every encoder, passed\n",
        "                through the vector-quantization table.\n",
        "                Ordered from bottom to top level.\n",
        "        \"\"\"\n",
        "        all_encoded = [inputs]\n",
        "        all_vq_outs = []\n",
        "        total_vq_loss = 0.0\n",
        "        total_recon_loss = 0.0\n",
        "        for encoder in self.encoders:\n",
        "            encoded = encoder.encode(all_encoded[-1])\n",
        "            embedded, embedded_pt, _ = encoder.vq(encoded)\n",
        "            all_encoded.append(encoded)\n",
        "            all_vq_outs.append(embedded_pt)\n",
        "            total_vq_loss = total_vq_loss + vq_loss(encoded, embedded, commitment=commitment)\n",
        "        losses = []\n",
        "        reconstructions = []\n",
        "        for i, decoder in enumerate(self.decoders):\n",
        "            dec_inputs = all_vq_outs[::-1][:i + 1]\n",
        "            target = all_encoded[::-1][i + 1]\n",
        "            recon = decoder(dec_inputs)\n",
        "            reconstructions.append(recon)\n",
        "            print(recon.shape)\n",
        "            print(target.shape)\n",
        "            if target.dtype.is_floating_point:\n",
        "                recon_loss = torch.mean(torch.pow(recon - target.detach(), 2))\n",
        "            else:\n",
        "                recon_loss = F.cross_entropy(recon.view(-1, recon.shape[-1]), target.view(-1))\n",
        "            total_recon_loss = total_recon_loss + recon_loss\n",
        "            losses.append(recon_loss)\n",
        "        return {\n",
        "            'loss': total_vq_loss + total_recon_loss,\n",
        "            'losses': losses,\n",
        "            'reconstructions': reconstructions,\n",
        "            'embedded': all_vq_outs,\n",
        "        }\n",
        "\n",
        "    def revive_dead_entries(self):\n",
        "        \"\"\"\n",
        "        Revive dead entries from all of the VQ layers.\n",
        "        Only call this once the encoders have all been\n",
        "        through a forward pass in training mode.\n",
        "        \"\"\"\n",
        "        for enc in self.encoders:\n",
        "            enc.vq.revive_dead_entries()\n",
        "\n",
        "    def full_reconstructions(self, inputs):\n",
        "        \"\"\"\n",
        "        Compute reconstructions of the inputs using all\n",
        "        the different layers of the hierarchy.\n",
        "        The first reconstruction uses only information\n",
        "        from the top-level codes, the second uses only\n",
        "        information from the top-level and second-to-top\n",
        "        level codes, etc.\n",
        "        This is not forward(inputs)['reconstructions'],\n",
        "        since said reconstructions are simply each level's\n",
        "        reconstruction of the next level's features.\n",
        "        Instead, full_reconstructions reconstructs the\n",
        "        original inputs.\n",
        "        \"\"\"\n",
        "        terms = self(inputs)\n",
        "        layer_recons = []\n",
        "        for encoder, recon in zip(self.encoders[:-1][::-1], terms['reconstructions'][:-1]):\n",
        "            _, embedded_pt, _ = encoder.vq(recon)\n",
        "            layer_recons.append(embedded_pt)\n",
        "        hierarchy_size = len(self.decoders)\n",
        "        results = []\n",
        "        for i in range(hierarchy_size - 1):\n",
        "            num_actual = i + 1\n",
        "            dec_in = terms['embedded'][-num_actual:][::-1] + layer_recons[num_actual - 1:]\n",
        "            results.append(self.decoders[-1](dec_in))\n",
        "        results.append(terms['reconstructions'][-1])\n",
        "        return results\n",
        "\n",
        "\n",
        "def _make_residual(channels):\n",
        "    return nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(channels, channels, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(channels, channels, 1),\n",
        "    )\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from .vq import embedding_distances\n",
        "\n",
        "def test_embedding_distances():\n",
        "    dictionary = torch.randn(15, 7)\n",
        "    tensor = torch.randn(3, 3, 7)\n",
        "    with torch.no_grad():\n",
        "        actual = embedding_distances(dictionary, tensor).numpy()\n",
        "        print(actual.shape)\n",
        "        expected = naive_embedding_distances(dictionary, tensor).numpy()\n",
        "        print(expected.shape)\n",
        "        assert np.allclose(actual, expected, atol=1e-4)\n",
        "\n",
        "\n",
        "def naive_embedding_distances(dictionary, tensor):\n",
        "    return torch.sum(torch.pow(tensor[..., None, :] - dictionary, 2), dim=-1)\n",
        "test_embedding_distances()\n",
        "(3, 3, 15)\n",
        "(3, 3, 15)\n",
        "\"\"\"\n",
        "An implementation of multi-head attention, based off of\n",
        "https://github.com/unixpickle/xformer\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PixelAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention layer that operates on images.\n",
        "    Args:\n",
        "        num_channels: the input image depth.\n",
        "        num_heads: the number of attention heads.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedAttention(num_channels, num_heads=num_heads)\n",
        "\n",
        "    def forward(self, *images, conds=None):\n",
        "        \"\"\"\n",
        "        Apply masked attention to a batch of images.\n",
        "        Args:\n",
        "            images: one or more [N x C x H x W] Tensors.\n",
        "            conds: ignored. Here for compatibility with\n",
        "              the PixelCNN aggregator.\n",
        "        Returns:\n",
        "            A new list of [N x C x H x W] Tensors.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for image in images:\n",
        "            batch, num_channels, height, width = image.shape\n",
        "            result = image.permute(0, 2, 3, 1)\n",
        "            result = result.view(batch, height * width, num_channels)\n",
        "            result = self.attention(result)\n",
        "            result = result.view(batch, height, width, num_channels)\n",
        "            result = result.permute(0, 3, 1, 2)\n",
        "            results.append(result + image)\n",
        "        if len(results) == 1:\n",
        "            return results[0]\n",
        "        return tuple(results)\n",
        "\n",
        "\n",
        "class MaskedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention layer that operates on sequences of the\n",
        "    shape [N x T x C], where N is the batch size, T is the\n",
        "    number of timesteps, and C is the number of channels.\n",
        "    Args:\n",
        "        num_channels: the number of channels in the input\n",
        "          sequences.\n",
        "        num_heads: the number of attention heads to use.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, num_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        assert not num_channels % num_heads, 'heads must evenly divide channels'\n",
        "        self.num_channels = num_channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.kqv_projection = nn.Linear(num_channels, num_channels * 3)\n",
        "        self.mix_heads = nn.Linear(num_channels, num_channels)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        \"\"\"\n",
        "        Apply masked multi-head attention.\n",
        "        Args:\n",
        "            sequence: an [N x T x C] Tensor.\n",
        "        Returns:\n",
        "            A new [N x T x C] Tensor.\n",
        "        \"\"\"\n",
        "        projected = self.kqv_projection(sequence)\n",
        "        kqv = torch.split(projected, self.num_channels, dim=-1)\n",
        "        keys, queries, values = [self._split_heads(x) for x in kqv]\n",
        "        logits = torch.bmm(queries, keys.permute(0, 2, 1))\n",
        "        logits /= math.sqrt(self.num_channels / self.num_heads)\n",
        "        logits += self._logit_mask(sequence.shape[1])\n",
        "        weights = F.softmax(logits, dim=-1)\n",
        "        weighted_sum = torch.bmm(weights, values)\n",
        "        combined = self._combine_heads(weighted_sum)\n",
        "        return self.mix_heads(combined)\n",
        "\n",
        "    def _split_heads(self, batch):\n",
        "        \"\"\"\n",
        "        Split up the channels in a batch into groups, one\n",
        "        per head.\n",
        "        Args:\n",
        "            batch: an [N x T x C] Tensor.\n",
        "        Returns:\n",
        "            An [N*H x T x C/H] Tensor.\n",
        "        \"\"\"\n",
        "        batch_size = batch.shape[0]\n",
        "        num_steps = batch.shape[1]\n",
        "        split_channels = self.num_channels // self.num_heads\n",
        "        batch = batch.view(batch_size, num_steps, self.num_heads, split_channels)\n",
        "        batch = batch.permute(0, 2, 1, 3).contiguous()\n",
        "        batch = batch.view(batch_size * self.num_heads, num_steps, split_channels)\n",
        "        return batch\n",
        "\n",
        "    def _combine_heads(self, batch):\n",
        "        \"\"\"\n",
        "        Perform the inverse of _split_heads().\n",
        "        Args:\n",
        "            batch: an [N*H x T x C/H] Tensor.\n",
        "        Returns:\n",
        "            An [N x T x C] Tensor.\n",
        "        \"\"\"\n",
        "        batch_size = batch.shape[0] // self.num_heads\n",
        "        num_steps = batch.shape[1]\n",
        "        split_channels = self.num_channels // self.num_heads\n",
        "        batch = batch.view(batch_size, self.num_heads, num_steps, split_channels)\n",
        "        batch = batch.permute(0, 2, 1, 3).contiguous()\n",
        "        batch = batch.view(batch_size, num_steps, self.num_channels)\n",
        "        return batch\n",
        "\n",
        "    def _logit_mask(self, num_steps):\n",
        "        row_indices = np.arange(num_steps)[:, None]\n",
        "        col_indices = np.arange(num_steps)[None]\n",
        "        upper = (row_indices >= col_indices)\n",
        "        mask = np.where(upper, 0, -np.inf).astype(np.float32)\n",
        "        return torch.from_numpy(mask).to(next(self.parameters()).device)\n",
        "\"\"\"\n",
        "An implementation of the Gated PixelCNN from\n",
        "https://arxiv.org/abs/1606.05328.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PixelCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A PixelCNN is a stack of PixelConv layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        for i, layer in enumerate(layers):\n",
        "            self.add_module('layer_%d' % i, layer)\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, images, conds=None):\n",
        "        \"\"\"\n",
        "        Apply the stack of PixelConv layers.\n",
        "        It is assumed that the first layer is a\n",
        "        PixelConvA, and the rest are PixelConvB's.\n",
        "        This way, the first layer takes one input and the\n",
        "        rest take two.\n",
        "        Returns:\n",
        "            A tuple (vertical, horizontal), one for each\n",
        "              of the two directional stacks.\n",
        "        \"\"\"\n",
        "        outputs = self.layers[0](images, conds=conds)\n",
        "        for layer in self.layers[1:]:\n",
        "            outputs = layer(*outputs, conds=conds)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PixelConv(nn.Module):\n",
        "    \"\"\"\n",
        "    An abstract base class for PixelCNN layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, depth_in, depth_out, cond_depth=None, horizontal=2, vertical=2):\n",
        "        super().__init__()\n",
        "        self.depth_in = depth_in\n",
        "        self.depth_out = depth_out\n",
        "        self.horizontal = horizontal\n",
        "        self.vertical = vertical\n",
        "\n",
        "        self._init_directional_convs()\n",
        "        self.vert_to_horiz = nn.Conv2d(depth_out * 2, depth_out * 2, 1)\n",
        "        self.cond_layer = None\n",
        "        if cond_depth is not None:\n",
        "            self.cond_layer = nn.Linear(cond_depth, depth_out * 4)\n",
        "\n",
        "    def _init_directional_convs(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _run_stacks(self, vert_in, horiz_in, conds):\n",
        "        vert_out = self._run_padded_vertical(vert_in)\n",
        "        horiz_out = self._run_padded_horizontal(horiz_in)\n",
        "        horiz_out = horiz_out + self.vert_to_horiz(vert_out)\n",
        "\n",
        "        if conds is not None:\n",
        "            cond_bias = self._compute_cond_bias(conds)\n",
        "            vert_out = vert_out + cond_bias[:, :self.depth_out*2]\n",
        "            horiz_out = horiz_out + cond_bias[:, self.depth_out*2:]\n",
        "\n",
        "        vert_out = gated_activation(vert_out)\n",
        "        horiz_out = gated_activation(horiz_out)\n",
        "        return vert_out, horiz_out\n",
        "\n",
        "    def _run_padded_vertical(self, vert_in):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _run_padded_horizontal(self, horiz_in):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _compute_cond_bias(self, conds):\n",
        "        if len(conds.shape) == 2:\n",
        "            outputs = self.cond_layer(conds)\n",
        "            return outputs.view(-1, outputs.shape[1], 1, 1)\n",
        "        assert len(conds.shape) == 4\n",
        "        conds_perm = conds.permute(0, 2, 3, 1)\n",
        "        outputs = self.cond_layer(conds_perm)\n",
        "        return outputs.permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class PixelConvA(PixelConv):\n",
        "    \"\"\"\n",
        "    The first layer in a PixelCNN. This layer is unlike\n",
        "    the other layers, in that it does not allow the stack\n",
        "    to see the current pixel.\n",
        "    Args:\n",
        "        depth_in: the number of input filters.\n",
        "        depth_out: the number of output filters.\n",
        "        cond_depth: the number of conditioning channels.\n",
        "          If None, this is an unconditional model.\n",
        "        horizontal: the receptive field of the horizontal\n",
        "          stack.\n",
        "        vertical: the receptive field of the vertical\n",
        "          stack.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, depth_in, depth_out, cond_depth=None, horizontal=2, vertical=2):\n",
        "        super().__init__(depth_in, depth_out, cond_depth=cond_depth, horizontal=2, vertical=2)\n",
        "\n",
        "    def forward(self, images, conds=None):\n",
        "        \"\"\"\n",
        "        Apply the layer to some images, producing latents.\n",
        "        Args:\n",
        "            images: an NCHW batch of images.\n",
        "            conds: an optional conditioning value. If set,\n",
        "              either an NCHW Tensor or an NxM Tensor.\n",
        "        Returns:\n",
        "            A tuple (vertical, horizontal), one for each\n",
        "              of the two directional stacks.\n",
        "        \"\"\"\n",
        "        return self._run_stacks(images, images, conds)\n",
        "\n",
        "    def _init_directional_convs(self):\n",
        "        self.vertical_conv = nn.Conv2d(self.depth_in, self.depth_out * 2,\n",
        "                                       (self.vertical, self.horizontal*2 + 1))\n",
        "        self.horizontal_conv = nn.Conv2d(self.depth_in, self.depth_out * 2, (1, self.horizontal))\n",
        "\n",
        "    def _run_padded_vertical(self, vert_in):\n",
        "        vert_pad = (self.horizontal, self.horizontal, self.vertical, 0)\n",
        "        return self.vertical_conv(F.pad(vert_in, vert_pad))[:, :, :-1, :]\n",
        "\n",
        "    def _run_padded_horizontal(self, horiz_in):\n",
        "        return self.horizontal_conv(F.pad(horiz_in, (self.horizontal, 0, 0, 0)))[:, :, :, :-1]\n",
        "\n",
        "\n",
        "class PixelConvB(PixelConv):\n",
        "    \"\"\"\n",
        "    Any layer except the first in a PixelCNN.\n",
        "    Args:\n",
        "        depth_in: the number of input filters.\n",
        "        cond_depth: the number of conditioning channels.\n",
        "          If None, this is an unconditional model.\n",
        "        horizontal: the receptive field of the horizontal\n",
        "          stack.\n",
        "        vertical: the receptive field of the vertical\n",
        "          stack.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, depth_in, cond_depth=None, norm=False, horizontal=2, vertical=2):\n",
        "        super().__init__(depth_in, depth_in, cond_depth=cond_depth, horizontal=horizontal,\n",
        "                         vertical=vertical)\n",
        "        self.horiz_residual = nn.Conv2d(depth_in, depth_in, 1)\n",
        "        self.vert_norm = lambda x: x\n",
        "        self.horiz_norm = lambda x: x\n",
        "        if norm:\n",
        "            self.vert_norm = ChannelNorm(depth_in)\n",
        "            self.horiz_norm = ChannelNorm(depth_in)\n",
        "\n",
        "    def forward(self, vert_in, horiz_in, conds=None):\n",
        "        \"\"\"\n",
        "        Apply the layer to the outputs of previous\n",
        "        vertical and horizontal stacks.\n",
        "        Args:\n",
        "            vert_in: an NCHW Tensor.\n",
        "            horiz_in: an NCHW Tensor.\n",
        "            conds: an optional conditioning value. If set,\n",
        "              either an NCHW Tensor or an NxM Tensor.\n",
        "        Returns:\n",
        "            A tuple (vertical, horizontal), one for each\n",
        "              of the two directional stacks.\n",
        "        \"\"\"\n",
        "        vert_out, horiz_out = self._run_stacks(vert_in, horiz_in, conds)\n",
        "        horiz_out = horiz_in + self.horiz_norm(self.horiz_residual(horiz_out))\n",
        "        return self.vert_norm(vert_out), horiz_out\n",
        "\n",
        "    def _init_directional_convs(self):\n",
        "        self.vertical_conv = nn.Conv2d(self.depth_in, self.depth_out * 2,\n",
        "                                       (self.vertical + 1, self.horizontal*2 + 1))\n",
        "        self.horizontal_conv = nn.Conv2d(self.depth_in, self.depth_out * 2,\n",
        "                                         (1, self.horizontal + 1))\n",
        "\n",
        "    def _run_padded_vertical(self, vert_in):\n",
        "        vert_pad = (self.horizontal, self.horizontal, self.vertical, 0)\n",
        "        return self.vertical_conv(F.pad(vert_in, vert_pad))\n",
        "\n",
        "    def _run_padded_horizontal(self, horiz_in):\n",
        "        return self.horizontal_conv(F.pad(horiz_in, (self.horizontal, 0, 0, 0)))\n",
        "\n",
        "\n",
        "class ChannelNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which applies layer normalization to the\n",
        "    channels at each spacial location separately.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm((num_channels,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()\n",
        "        x = self.norm(x)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n",
        "\n",
        "\n",
        "def gated_activation(outputs):\n",
        "    depth = outputs.shape[1] // 2\n",
        "    tanh = torch.tanh(outputs[:, :depth])\n",
        "    sigmoid = torch.sigmoid(outputs[:, depth:])\n",
        "    return tanh * sigmoid\n",
        "from math import cos, pi, floor, sin\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class CosineLR(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, lr_min, lr_max, step_size):\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max\n",
        "        self.step_size = step_size\n",
        "        self.iteration = 0\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (\n",
        "            1 + cos(self.iteration / self.step_size * pi)\n",
        "        )\n",
        "        self.iteration += 1\n",
        "\n",
        "        if self.iteration == self.step_size:\n",
        "            self.iteration = 0\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class PowerLR(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, lr_min, lr_max, warmup):\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max\n",
        "        self.warmup = warmup\n",
        "        self.iteration = 0\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.iteration < self.warmup:\n",
        "            lr = (\n",
        "                self.lr_min + (self.lr_max - self.lr_min) / self.warmup * self.iteration\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            lr = self.lr_max * (self.iteration - self.warmup + 1) ** -0.5\n",
        "\n",
        "        self.iteration += 1\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class SineLR(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, lr_min, lr_max, step_size):\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max\n",
        "        self.step_size = step_size\n",
        "        self.iteration = 0\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = self.lr_min + (self.lr_max - self.lr_min) * sin(\n",
        "            self.iteration / self.step_size * pi\n",
        "        )\n",
        "        self.iteration += 1\n",
        "\n",
        "        if self.iteration == self.step_size:\n",
        "            self.iteration = 0\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class LinearLR(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, lr_min, lr_max, warmup, step_size):\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max\n",
        "        self.step_size = step_size\n",
        "        self.warmup = warmup\n",
        "        self.iteration = 0\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.iteration < self.warmup:\n",
        "            lr = self.lr_max\n",
        "\n",
        "        else:\n",
        "            lr = self.lr_max + (self.iteration - self.warmup) * (\n",
        "                self.lr_min - self.lr_max\n",
        "            ) / (self.step_size - self.warmup)\n",
        "        self.iteration += 1\n",
        "\n",
        "        if self.iteration == self.step_size:\n",
        "            self.iteration = 0\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class CLR(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, lr_min, lr_max, step_size):\n",
        "        self.epoch = 0\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max\n",
        "        self.current_lr = lr_min\n",
        "        self.step_size = step_size\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        cycle = floor(1 + self.epoch / (2 * self.step_size))\n",
        "        x = abs(self.epoch / self.step_size - 2 * cycle + 1)\n",
        "        lr = self.lr_min + (self.lr_max - self.lr_min) * max(0, 1 - x)\n",
        "        self.current_lr = lr\n",
        "\n",
        "        self.epoch += 1\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class Warmup(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, model_dim, factor=1, warmup=16000):\n",
        "        self.optimizer = optimizer\n",
        "        self.model_dim = model_dim\n",
        "        self.factor = factor\n",
        "        self.warmup = warmup\n",
        "        self.iteration = 0\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        self.iteration += 1\n",
        "        lr = (\n",
        "            self.factor\n",
        "            * self.model_dim ** (-0.5)\n",
        "            * min(self.iteration ** (-0.5), self.iteration * self.warmup ** (-1.5))\n",
        "        )\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "# Copyright 2019 fastai\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "# Borrowed from https://github.com/fastai/fastai and changed to make it runs like PyTorch lr scheduler\n",
        "\n",
        "\n",
        "class CycleAnnealScheduler:\n",
        "    def __init__(\n",
        "        self, optimizer, lr_max, lr_divider, cut_point, step_size, momentum=None\n",
        "    ):\n",
        "        self.lr_max = lr_max\n",
        "        self.lr_divider = lr_divider\n",
        "        self.cut_point = step_size // cut_point\n",
        "        self.step_size = step_size\n",
        "        self.iteration = 0\n",
        "        self.cycle_step = int(step_size * (1 - cut_point / 100) / 2)\n",
        "        self.momentum = momentum\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.iteration > 2 * self.cycle_step:\n",
        "            cut = (self.iteration - 2 * self.cycle_step) / (\n",
        "                self.step_size - 2 * self.cycle_step\n",
        "            )\n",
        "            lr = self.lr_max * (1 + (cut * (1 - 100) / 100)) / self.lr_divider\n",
        "\n",
        "        elif self.iteration > self.cycle_step:\n",
        "            cut = 1 - (self.iteration - self.cycle_step) / self.cycle_step\n",
        "            lr = self.lr_max * (1 + cut * (self.lr_divider - 1)) / self.lr_divider\n",
        "\n",
        "        else:\n",
        "            cut = self.iteration / self.cycle_step\n",
        "            lr = self.lr_max * (1 + cut * (self.lr_divider - 1)) / self.lr_divider\n",
        "\n",
        "        return lr\n",
        "\n",
        "    def get_momentum(self):\n",
        "        if self.iteration > 2 * self.cycle_step:\n",
        "            momentum = self.momentum[0]\n",
        "\n",
        "        elif self.iteration > self.cycle_step:\n",
        "            cut = 1 - (self.iteration - self.cycle_step) / self.cycle_step\n",
        "            momentum = self.momentum[0] + cut * (self.momentum[1] - self.momentum[0])\n",
        "\n",
        "        else:\n",
        "            cut = self.iteration / self.cycle_step\n",
        "            momentum = self.momentum[0] + cut * (self.momentum[1] - self.momentum[0])\n",
        "\n",
        "        return momentum\n",
        "\n",
        "    def step(self):\n",
        "        lr = self.get_lr()\n",
        "\n",
        "        if self.momentum is not None:\n",
        "            momentum = self.get_momentum()\n",
        "\n",
        "        self.iteration += 1\n",
        "\n",
        "        if self.iteration == self.step_size:\n",
        "            self.iteration = 0\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            group['lr'] = lr\n",
        "\n",
        "            if self.momentum is not None:\n",
        "                group['betas'] = (momentum, group['betas'][1])\n",
        "\n",
        "        return lr\n",
        "\n",
        "\n",
        "def anneal_linear(start, end, proportion):\n",
        "    return start + proportion * (end - start)\n",
        "\n",
        "\n",
        "def anneal_cos(start, end, proportion):\n",
        "    cos_val = cos(pi * proportion) + 1\n",
        "\n",
        "    return end + (start - end) / 2 * cos_val\n",
        "\n",
        "\n",
        "class Phase:\n",
        "    def __init__(self, start, end, n_iter, anneal_fn):\n",
        "        self.start, self.end = start, end\n",
        "        self.n_iter = n_iter\n",
        "        self.anneal_fn = anneal_fn\n",
        "        self.n = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.n += 1\n",
        "\n",
        "        return self.anneal_fn(self.start, self.end, self.n / self.n_iter)\n",
        "\n",
        "    def reset(self):\n",
        "        self.n = 0\n",
        "\n",
        "    @property\n",
        "    def is_done(self):\n",
        "        return self.n >= self.n_iter\n",
        "\n",
        "\n",
        "class CycleScheduler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        lr_max,\n",
        "        n_iter,\n",
        "        momentum=(0.95, 0.85),\n",
        "        divider=25,\n",
        "        warmup_proportion=0.3,\n",
        "        phase=('linear', 'cos'),\n",
        "    ):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        phase1 = int(n_iter * warmup_proportion)\n",
        "        phase2 = n_iter - phase1\n",
        "        lr_min = lr_max / divider\n",
        "\n",
        "        phase_map = {'linear': anneal_linear, 'cos': anneal_cos}\n",
        "\n",
        "        self.lr_phase = [\n",
        "            Phase(lr_min, lr_max, phase1, phase_map[phase[0]]),\n",
        "            Phase(lr_max, lr_min / 1e4, phase2, phase_map[phase[1]]),\n",
        "        ]\n",
        "\n",
        "        self.momentum = momentum\n",
        "\n",
        "        if momentum is not None:\n",
        "            mom1, mom2 = momentum\n",
        "            self.momentum_phase = [\n",
        "                Phase(mom1, mom2, phase1, phase_map[phase[0]]),\n",
        "                Phase(mom2, mom1, phase2, phase_map[phase[1]]),\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            self.momentum_phase = []\n",
        "\n",
        "        self.phase = 0\n",
        "\n",
        "    def step(self):\n",
        "        lr = self.lr_phase[self.phase].step()\n",
        "\n",
        "        if self.momentum is not None:\n",
        "            momentum = self.momentum_phase[self.phase].step()\n",
        "\n",
        "        else:\n",
        "            momentum = None\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            group['lr'] = lr\n",
        "\n",
        "            if self.momentum is not None:\n",
        "                if 'betas' in group:\n",
        "                    group['betas'] = (momentum, group['betas'][1])\n",
        "\n",
        "                else:\n",
        "                    group['momentum'] = momentum\n",
        "\n",
        "        if self.lr_phase[self.phase].is_done:\n",
        "            self.phase += 1\n",
        "\n",
        "        if self.phase >= len(self.lr_phase):\n",
        "            for phase in self.lr_phase:\n",
        "                phase.reset()\n",
        "\n",
        "            for phase in self.momentum_phase:\n",
        "                phase.reset()\n",
        "\n",
        "            self.phase = 0\n",
        "\n",
        "        return lr, momentum\n",
        "\n",
        "\n",
        "class LRFinder(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, lr_min, lr_max, step_size, linear=False):\n",
        "        ratio = lr_max / lr_min\n",
        "        self.linear = linear\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_mult = (ratio / step_size) if linear else ratio ** (1 / step_size)\n",
        "        self.iteration = 0\n",
        "        self.lrs = []\n",
        "        self.losses = []\n",
        "\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = (\n",
        "            self.lr_mult * self.iteration\n",
        "            if self.linear\n",
        "            else self.lr_mult ** self.iteration\n",
        "        )\n",
        "        lr = self.lr_min + lr if self.linear else self.lr_min * lr\n",
        "\n",
        "        self.iteration += 1\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        return [lr for base_lr in self.base_lrs]\n",
        "\n",
        "    def record(self, loss):\n",
        "        self.losses.append(loss)\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            for lr, loss in zip(self.lrs, self.losses):\n",
        "                f.write('{},{}\\n'.format(lr, loss))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:47:06.940176Z",
          "iopub.execute_input": "2022-02-16T07:47:06.940471Z",
          "iopub.status.idle": "2022-02-16T07:47:07.361579Z",
          "shell.execute_reply.started": "2022-02-16T07:47:06.940438Z",
          "shell.execute_reply": "2022-02-16T07:47:07.36062Z"
        },
        "trusted": true,
        "id": "hPqjCVLbdVw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Models for hierarchical image generation.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def make_vae():\n",
        "#     encoders = [QuarterEncoder(3, 128, 512), HalfEncoder(128, 128, 512)]\n",
        "#     decoders = [HalfDecoder(128, 128), HalfQuarterDecoder(128, 3)]\n",
        "    encoders = [QuarterEncoder(3, 64, 128), HalfEncoder(64, 64, 128)]\n",
        "    decoders = [HalfDecoder(64, 64), HalfQuarterDecoder(64, 3)]\n",
        "    return VQVAE(encoders, decoders)\n",
        "\n",
        "\n",
        "class TopPrior(nn.Module):\n",
        "    def __init__(self, depth=64, num_heads=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(128, depth)\n",
        "        self.pixel_cnn = PixelCNN(\n",
        "            PixelConvA(depth, depth),\n",
        "\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelAttention(depth, num_heads=num_heads),\n",
        "\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelAttention(depth, num_heads=num_heads),\n",
        "\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelAttention(depth, num_heads=num_heads),\n",
        "\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelConvB(depth, norm=True),\n",
        "            PixelAttention(depth, num_heads=num_heads),\n",
        "        )\n",
        "        self.out_stack = nn.Sequential(\n",
        "            nn.Conv2d(depth * 2, depth, 1),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            Residual1x1(depth),\n",
        "            nn.Conv2d(depth, 256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        out1, out2 = self.pixel_cnn(x)\n",
        "        return self.out_stack(torch.cat([out1, out2], dim=1))\n",
        "\n",
        "\n",
        "class BottomPrior(nn.Module):\n",
        "    def __init__(self, depth=64, num_heads=2):\n",
        "        super().__init__()\n",
        "        self.embed_top = nn.Embedding(128, depth)\n",
        "        self.embed_bottom = nn.Embedding(128, depth)\n",
        "        self.cond_stack = nn.Sequential(\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            Residual3x3(depth),\n",
        "            nn.ConvTranspose2d(depth, depth, 4, stride=2, padding=1),\n",
        "        )\n",
        "        self.pixel_cnn = PixelCNN(\n",
        "            PixelConvA(depth, depth, cond_depth=depth),\n",
        "\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
        "        )\n",
        "        self.out_stack = nn.Sequential(\n",
        "            nn.Conv2d(depth * 2, depth, 1),\n",
        "            nn.Conv2d(depth, 128, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, bottom, top):\n",
        "        conds = self.embed_top(top)\n",
        "        conds = conds.permute(0, 3, 1, 2).contiguous()\n",
        "        conds = self.cond_stack(conds)\n",
        "\n",
        "        out = self.embed_bottom(bottom)\n",
        "        out = out.permute(0, 3, 1, 2).contiguous()\n",
        "        out1, out2 = self.pixel_cnn(out, conds=conds)\n",
        "        return self.out_stack(torch.cat([out1, out2], dim=1))\n",
        "\n",
        "\n",
        "class Residual1x1(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, 1)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, 1)\n",
        "        self.norm = ChannelNorm(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        return inputs + self.norm(x)\n",
        "\n",
        "\n",
        "class Residual3x3(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, padding=1)\n",
        "        self.norm = ChannelNorm(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        return inputs + self.norm(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:47:08.293174Z",
          "iopub.execute_input": "2022-02-16T07:47:08.293713Z",
          "iopub.status.idle": "2022-02-16T07:47:08.332204Z",
          "shell.execute_reply.started": "2022-02-16T07:47:08.293676Z",
          "shell.execute_reply": "2022-02-16T07:47:08.331019Z"
        },
        "trusted": true,
        "id": "Y3RDiolQdVxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = make_vae()\n",
        "s = 0\n",
        "for el in a.parameters():\n",
        "  s += len(el)\n",
        "s"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:47:09.071014Z",
          "iopub.execute_input": "2022-02-16T07:47:09.071362Z",
          "iopub.status.idle": "2022-02-16T07:47:09.097144Z",
          "shell.execute_reply.started": "2022-02-16T07:47:09.071328Z",
          "shell.execute_reply": "2022-02-16T07:47:09.096169Z"
        },
        "trusted": true,
        "id": "Vw3yrNP2dVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train a hierarchical VQ-VAE on 256x256 images.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "VAE_PATH = 'vae.pt'\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def main_vae():\n",
        "    epochs = 5000\n",
        "    batch_size = 64\n",
        "    device = torch.device(DEVICE)\n",
        "    model = make_vae()\n",
        "    if os.path.exists(VAE_PATH):\n",
        "        model.load_state_dict(torch.load(VAE_PATH, map_location=DEVICE))\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    schedule = CycleScheduler(optimizer, 0.001, batch_size * epochs, momentum=(0.95, 0.85),)\n",
        "    data = train_loader\n",
        "    for i in itertools.count():\n",
        "        images = next(iter(data)).to(device)\n",
        "        terms = model(images)\n",
        "        mse, mse_top = terms['losses'][-1].item(), terms['losses'][0].item()\n",
        "        print('step %d: mse=%f mse_top=%f' %\n",
        "              (i, mse, mse_top))\n",
        "        optimizer.zero_grad()\n",
        "        terms['loss'].backward()\n",
        "        schedule.step()\n",
        "        optimizer.step()\n",
        "        model.revive_dead_entries()\n",
        "        \n",
        "        if i == epochs: \n",
        "            torch.save(model.state_dict(), VAE_PATH)\n",
        "            save_reconstructions(model, images)\n",
        "            break\n",
        "\n",
        "def save_reconstructions(vae, images):\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        recons = [torch.clamp(x, 0, 1).permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "                  for x in vae.full_reconstructions(images)]\n",
        "    vae.train()\n",
        "    top_recons, real_recons = recons\n",
        "    images = images.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "\n",
        "    columns = np.concatenate([top_recons, real_recons, images], axis=-2)\n",
        "    columns = np.concatenate(columns, axis=0)\n",
        "    Image.fromarray((columns * 255).astype('uint8')).save('reconstructions.png')\n",
        "\n",
        "main_vae()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-16T07:47:09.844281Z",
          "iopub.execute_input": "2022-02-16T07:47:09.844591Z",
          "iopub.status.idle": "2022-02-16T07:47:10.194982Z",
          "shell.execute_reply.started": "2022-02-16T07:47:09.844557Z",
          "shell.execute_reply": "2022-02-16T07:47:10.193262Z"
        },
        "trusted": true,
        "id": "CCIGTsNhdVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NocmlR7kdVxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7uRqrHkqdVxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}